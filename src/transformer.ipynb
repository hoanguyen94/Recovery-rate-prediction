{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Callable, cast, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# import typing as ty\n",
    "from torch import Tensor\n",
    "import torch.nn.init as nn_init\n",
    "import statistics\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from utils import calculate_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f01bf92e930>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"../data/data_removing_na.xlsx\"\n",
    "MODEL_PATH = '../output/transformer'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE_LIST = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rr1_30</th>\n",
       "      <th>currency</th>\n",
       "      <th>seniorioty_adj</th>\n",
       "      <th>coupon rate</th>\n",
       "      <th>domicile_country</th>\n",
       "      <th>exchange_country</th>\n",
       "      <th>Industry_sector</th>\n",
       "      <th>Industry_group</th>\n",
       "      <th>Industry_subgroup</th>\n",
       "      <th>event_type</th>\n",
       "      <th>...</th>\n",
       "      <th>PD_55_pd</th>\n",
       "      <th>PD_56_pd</th>\n",
       "      <th>PD_57_pd</th>\n",
       "      <th>PD_58_pd</th>\n",
       "      <th>PD_59_pd</th>\n",
       "      <th>PD_60_pd</th>\n",
       "      <th>DTD</th>\n",
       "      <th>NI_Over_TA</th>\n",
       "      <th>Size</th>\n",
       "      <th>defaulted_in_last_6_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.259908</td>\n",
       "      <td>USD</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>9.000</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Retail &amp; Whsle - Discretionary</td>\n",
       "      <td>E-Commerce Discretionary</td>\n",
       "      <td>Bankruptcy Filing</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396731</td>\n",
       "      <td>0.397453</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.398819</td>\n",
       "      <td>0.399467</td>\n",
       "      <td>0.400092</td>\n",
       "      <td>-0.732815</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.852484</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.032729</td>\n",
       "      <td>USD</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>5.750</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Facilities &amp; Svcs</td>\n",
       "      <td>Default Corp Action</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957454</td>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.957480</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.957503</td>\n",
       "      <td>0.957514</td>\n",
       "      <td>-1.666262</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-1.186347</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972400</td>\n",
       "      <td>USD</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>5.675</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Retail &amp; Whsle - Discretionary</td>\n",
       "      <td>Wholesale - Discretionary</td>\n",
       "      <td>Default Corp Action</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568169</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.047416</td>\n",
       "      <td>CHF</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>0.125</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Retail &amp; Whsle - Discretionary</td>\n",
       "      <td>Wholesale - Discretionary</td>\n",
       "      <td>Default Corp Action</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568169</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.848872</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>1.750</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Products</td>\n",
       "      <td>Electrical Equipment</td>\n",
       "      <td>Bankruptcy Filing</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130285</td>\n",
       "      <td>0.130688</td>\n",
       "      <td>0.131081</td>\n",
       "      <td>0.131465</td>\n",
       "      <td>0.131840</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>-0.768857</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-1.946507</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rr1_30 currency                 seniorioty_adj  coupon rate  \\\n",
       "0  0.259908      USD  Senior Subordinated Unsecured        9.000   \n",
       "1  0.032729      USD  Senior Subordinated Unsecured        5.750   \n",
       "2  0.972400      USD                      Unsecured        5.675   \n",
       "3  1.047416      CHF                      Unsecured        0.125   \n",
       "4  0.848872      JPY                      Unsecured        1.750   \n",
       "\n",
       "  domicile_country exchange_country         Industry_sector  \\\n",
       "0    United States    United States  Consumer Discretionary   \n",
       "1    United States    United States             Health Care   \n",
       "2      South Korea      South Korea  Consumer Discretionary   \n",
       "3      South Korea      South Korea  Consumer Discretionary   \n",
       "4            Japan            Japan             Industrials   \n",
       "\n",
       "                   Industry_group              Industry_subgroup  \\\n",
       "0  Retail & Whsle - Discretionary       E-Commerce Discretionary   \n",
       "1                     Health Care  Health Care Facilities & Svcs   \n",
       "2  Retail & Whsle - Discretionary      Wholesale - Discretionary   \n",
       "3  Retail & Whsle - Discretionary      Wholesale - Discretionary   \n",
       "4             Industrial Products           Electrical Equipment   \n",
       "\n",
       "            event_type  ...  PD_55_pd  PD_56_pd  PD_57_pd  PD_58_pd  PD_59_pd  \\\n",
       "0    Bankruptcy Filing  ...  0.396731  0.397453  0.398148  0.398819  0.399467   \n",
       "1  Default Corp Action  ...  0.957454  0.957467  0.957480  0.957492  0.957503   \n",
       "2  Default Corp Action  ...  0.568169  0.568693  0.569197  0.569682  0.570150   \n",
       "3  Default Corp Action  ...  0.568169  0.568693  0.569197  0.569682  0.570150   \n",
       "4    Bankruptcy Filing  ...  0.130285  0.130688  0.131081  0.131465  0.131840   \n",
       "\n",
       "   PD_60_pd       DTD  NI_Over_TA      Size  defaulted_in_last_6_months  \n",
       "0  0.400092 -0.732815   -0.007137 -0.852484                       False  \n",
       "1  0.957514 -1.666262   -0.000286 -1.186347                       False  \n",
       "2  0.570600 -1.853366    0.000191  1.053677                       False  \n",
       "3  0.570600 -1.853366    0.000191  1.053677                       False  \n",
       "4  0.132206 -0.768857   -0.028058 -1.946507                       False  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 165)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['rr1_30']\n",
    "features = df.drop(columns=['rr1_30', 'rr1_7', 'rr2_7', 'rr2_30'], errors=\"ignore\") #drop labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currency                       object\n",
       "seniorioty_adj                 object\n",
       "coupon rate                   float64\n",
       "domicile_country               object\n",
       "exchange_country               object\n",
       "                               ...   \n",
       "PD_60_pd                      float64\n",
       "DTD                           float64\n",
       "NI_Over_TA                    float64\n",
       "Size                          float64\n",
       "defaulted_in_last_6_months       bool\n",
       "Length: 164, dtype: object"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = features.columns\n",
    "category_features = list(features.select_dtypes(include=['object', 'bool']).columns)\n",
    "non_category_features = [i for i in feature_list if i not in category_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(non_category_features))\n",
    "print(len(category_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce all categories to have type string\n",
    "features[category_features] = features[category_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical features in features\n",
    "label_encoders = {}\n",
    "mappings = {}\n",
    "\n",
    "for column in category_features:\n",
    "    le = LabelEncoder()\n",
    "    features[column] = le.fit_transform(features[column])\n",
    "    label_encoders[column] = le\n",
    "    mappings[column] = {index: label for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currency': {0: 'CAD',\n",
       "  1: 'CHF',\n",
       "  2: 'CNY',\n",
       "  3: 'EUR',\n",
       "  4: 'GBP',\n",
       "  5: 'HKD',\n",
       "  6: 'INR',\n",
       "  7: 'ISK',\n",
       "  8: 'JPY',\n",
       "  9: 'MYR',\n",
       "  10: 'NOK',\n",
       "  11: 'SEK',\n",
       "  12: 'SGD',\n",
       "  13: 'THB',\n",
       "  14: 'TWD',\n",
       "  15: 'USD'},\n",
       " 'seniorioty_adj': {0: 'Junior Unsecured or Junior Subordinated Unsecured',\n",
       "  1: 'Secured',\n",
       "  2: 'Senior Secured',\n",
       "  3: 'Senior Subordinated Unsecured',\n",
       "  4: 'Senior Unsecured',\n",
       "  5: 'Subordinated Unsecured',\n",
       "  6: 'Unsecured'},\n",
       " 'domicile_country': {0: 'Argentina',\n",
       "  1: 'Australia',\n",
       "  2: 'Bahamas',\n",
       "  3: 'Belgium',\n",
       "  4: 'Bermuda',\n",
       "  5: 'Canada',\n",
       "  6: 'Cayman Islands',\n",
       "  7: 'China',\n",
       "  8: 'Czech Republic',\n",
       "  9: 'Greece',\n",
       "  10: 'Hong Kong',\n",
       "  11: 'Iceland',\n",
       "  12: 'India',\n",
       "  13: 'Indonesia',\n",
       "  14: 'Japan',\n",
       "  15: 'Luxembourg',\n",
       "  16: 'Malaysia',\n",
       "  17: 'Mongolia',\n",
       "  18: 'Philippines',\n",
       "  19: 'Poland',\n",
       "  20: 'Singapore',\n",
       "  21: 'South Africa',\n",
       "  22: 'South Korea',\n",
       "  23: 'Taiwan',\n",
       "  24: 'Thailand',\n",
       "  25: 'United Kingdom',\n",
       "  26: 'United States'},\n",
       " 'exchange_country': {0: 'Australia',\n",
       "  1: 'China',\n",
       "  2: 'Hong Kong',\n",
       "  3: 'India',\n",
       "  4: 'Indonesia',\n",
       "  5: 'Japan',\n",
       "  6: 'Malaysia',\n",
       "  7: 'Philippines',\n",
       "  8: 'Singapore',\n",
       "  9: 'South Korea',\n",
       "  10: 'Taiwan',\n",
       "  11: 'Thailand',\n",
       "  12: 'United States'},\n",
       " 'Industry_sector': {0: 'Communications',\n",
       "  1: 'Consumer Discretionary',\n",
       "  2: 'Consumer Staples',\n",
       "  3: 'Energy',\n",
       "  4: 'Financials',\n",
       "  5: 'Health Care',\n",
       "  6: 'Industrials',\n",
       "  7: 'Materials',\n",
       "  8: 'Real Estate',\n",
       "  9: 'Technology',\n",
       "  10: 'Utilities'},\n",
       " 'Industry_group': {0: 'Banking',\n",
       "  1: 'Consumer Discretionary Products',\n",
       "  2: 'Consumer Discretionary Services',\n",
       "  3: 'Consumer Staple Products',\n",
       "  4: 'Health Care',\n",
       "  5: 'Industrial Products',\n",
       "  6: 'Industrial Services',\n",
       "  7: 'Insurance',\n",
       "  8: 'Materials',\n",
       "  9: 'Media',\n",
       "  10: 'Oil & Gas',\n",
       "  11: 'Real Estate',\n",
       "  12: 'Renewable Energy',\n",
       "  13: 'Retail & Wholesale - Staples',\n",
       "  14: 'Retail & Whsle - Discretionary',\n",
       "  15: 'Software & Tech Services',\n",
       "  16: 'Tech Hardware & Semiconductors',\n",
       "  17: 'Telecommunications',\n",
       "  18: 'Utilities'},\n",
       " 'Industry_subgroup': {0: 'Advertising & Marketing',\n",
       "  1: 'Apparel & Textile Products',\n",
       "  2: 'Automotive',\n",
       "  3: 'Banking',\n",
       "  4: 'Beverages',\n",
       "  5: 'Biotech & Pharma',\n",
       "  6: 'Cable & Satellite',\n",
       "  7: 'Chemicals',\n",
       "  8: 'Commercial Support Services',\n",
       "  9: 'Construction Materials',\n",
       "  10: 'Consumer Services',\n",
       "  11: 'Containers & Packaging',\n",
       "  12: 'E-Commerce Discretionary',\n",
       "  13: 'Electric Utilities',\n",
       "  14: 'Electrical Equipment',\n",
       "  15: 'Engineering & Construction',\n",
       "  16: 'Entertainment Content',\n",
       "  17: 'Food',\n",
       "  18: 'Forestry, Paper & Wood Products',\n",
       "  19: 'Gas & Water Utilities',\n",
       "  20: 'Health Care Facilities & Svcs',\n",
       "  21: 'Home & Office Products',\n",
       "  22: 'Home Construction',\n",
       "  23: 'Household Products',\n",
       "  24: 'Industrial Intermediate Prod',\n",
       "  25: 'Industrial Support Services',\n",
       "  26: 'Insurance',\n",
       "  27: 'Leisure Facilities & Services',\n",
       "  28: 'Leisure Products',\n",
       "  29: 'Machinery',\n",
       "  30: 'Medical Equipment & Devices',\n",
       "  31: 'Metals & Mining',\n",
       "  32: 'Oil & Gas Producers',\n",
       "  33: 'Oil & Gas Services & Equip',\n",
       "  34: 'Publishing & Broadcasting',\n",
       "  35: 'REIT',\n",
       "  36: 'Real Estate Owners & Developers',\n",
       "  37: 'Real Estate Services',\n",
       "  38: 'Renewable Energy',\n",
       "  39: 'Retail - Consumer Staples',\n",
       "  40: 'Retail - Discretionary',\n",
       "  41: 'Semiconductors',\n",
       "  42: 'Software',\n",
       "  43: 'Steel',\n",
       "  44: 'Technology Hardware',\n",
       "  45: 'Technology Services',\n",
       "  46: 'Telecommunications',\n",
       "  47: 'Transportation & Logistics',\n",
       "  48: 'Transportation Equipment',\n",
       "  49: 'Wholesale - Consumer Staples',\n",
       "  50: 'Wholesale - Discretionary'},\n",
       " 'event_type': {0: 'Bankruptcy Filing',\n",
       "  1: 'Default Corp Action',\n",
       "  2: 'Delisting'},\n",
       " 'event_type_subcategory_sum': {0: 'Bankruptcy',\n",
       "  1: 'Debt Restructuring',\n",
       "  2: 'Insolvency',\n",
       "  3: 'Liquidation',\n",
       "  4: 'Missing Coupon & principal payment',\n",
       "  5: 'Missing Coupon payment only',\n",
       "  6: 'Missing Interest payment',\n",
       "  7: 'Missing Loan payment',\n",
       "  8: 'Missing Principal payment',\n",
       "  9: 'Others',\n",
       "  10: 'Pre-Negotiated Chapter 11',\n",
       "  11: 'Protection',\n",
       "  12: 'Receivership',\n",
       "  13: 'Rehabilitation',\n",
       "  14: 'Restructuring'},\n",
       " 'defaulted_in_last_5_years': {0: 'False', 1: 'True'},\n",
       " 'defaulted_in_last_6_months': {0: 'False', 1: 'True'}}"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [len(le.classes_) for le in label_encoders.values()]\n",
    "d_numerical = len(non_category_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1293, 164)\n",
      "Training Labels Shape: (1293,)\n",
      "Testing Features Shape: (432, 164)\n",
      "Testing Labels Shape: (432,)\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test set\n",
    "test_size = 0.25\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency</th>\n",
       "      <th>seniorioty_adj</th>\n",
       "      <th>coupon rate</th>\n",
       "      <th>domicile_country</th>\n",
       "      <th>exchange_country</th>\n",
       "      <th>Industry_sector</th>\n",
       "      <th>Industry_group</th>\n",
       "      <th>Industry_subgroup</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_type_subcategory_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>PD_55_pd</th>\n",
       "      <th>PD_56_pd</th>\n",
       "      <th>PD_57_pd</th>\n",
       "      <th>PD_58_pd</th>\n",
       "      <th>PD_59_pd</th>\n",
       "      <th>PD_60_pd</th>\n",
       "      <th>DTD</th>\n",
       "      <th>NI_Over_TA</th>\n",
       "      <th>Size</th>\n",
       "      <th>defaulted_in_last_6_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>7.500</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691011</td>\n",
       "      <td>0.691382</td>\n",
       "      <td>0.691738</td>\n",
       "      <td>0.692080</td>\n",
       "      <td>0.692408</td>\n",
       "      <td>0.692725</td>\n",
       "      <td>-1.084433</td>\n",
       "      <td>-0.052027</td>\n",
       "      <td>-2.074964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183801</td>\n",
       "      <td>0.184996</td>\n",
       "      <td>0.186167</td>\n",
       "      <td>0.187313</td>\n",
       "      <td>0.188437</td>\n",
       "      <td>0.189539</td>\n",
       "      <td>-0.540409</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>0.864692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11.000</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278774</td>\n",
       "      <td>0.280216</td>\n",
       "      <td>0.281615</td>\n",
       "      <td>0.282972</td>\n",
       "      <td>0.284290</td>\n",
       "      <td>0.285571</td>\n",
       "      <td>0.754647</td>\n",
       "      <td>-0.010395</td>\n",
       "      <td>-0.342209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>9.125</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190375</td>\n",
       "      <td>0.191471</td>\n",
       "      <td>0.192544</td>\n",
       "      <td>0.193594</td>\n",
       "      <td>0.194622</td>\n",
       "      <td>0.195628</td>\n",
       "      <td>-0.242080</td>\n",
       "      <td>-0.022618</td>\n",
       "      <td>-2.808528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>9.250</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149501</td>\n",
       "      <td>0.150930</td>\n",
       "      <td>0.152327</td>\n",
       "      <td>0.153692</td>\n",
       "      <td>0.155027</td>\n",
       "      <td>0.156332</td>\n",
       "      <td>1.138686</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.085154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      currency  seniorioty_adj  coupon rate  domicile_country  \\\n",
       "755         15               4        7.500                26   \n",
       "588         15               6        6.000                13   \n",
       "585         15               2       11.000                26   \n",
       "1329        15               2        9.125                26   \n",
       "973         15               2        9.250                26   \n",
       "\n",
       "      exchange_country  Industry_sector  Industry_group  Industry_subgroup  \\\n",
       "755                 12                3              10                 32   \n",
       "588                  4                7               8                 31   \n",
       "585                 12                7               8                 31   \n",
       "1329                12                3              10                 33   \n",
       "973                 12                3              10                 32   \n",
       "\n",
       "      event_type  event_type_subcategory_sum  ...  PD_55_pd  PD_56_pd  \\\n",
       "755            1                           5  ...  0.691011  0.691382   \n",
       "588            1                           1  ...  0.183801  0.184996   \n",
       "585            1                           1  ...  0.278774  0.280216   \n",
       "1329           1                           4  ...  0.190375  0.191471   \n",
       "973            1                           9  ...  0.149501  0.150930   \n",
       "\n",
       "      PD_57_pd  PD_58_pd  PD_59_pd  PD_60_pd       DTD  NI_Over_TA      Size  \\\n",
       "755   0.691738  0.692080  0.692408  0.692725 -1.084433   -0.052027 -2.074964   \n",
       "588   0.186167  0.187313  0.188437  0.189539 -0.540409    0.017209  0.864692   \n",
       "585   0.281615  0.282972  0.284290  0.285571  0.754647   -0.010395 -0.342209   \n",
       "1329  0.192544  0.193594  0.194622  0.195628 -0.242080   -0.022618 -2.808528   \n",
       "973   0.152327  0.153692  0.155027  0.156332  1.138686    0.000033 -0.085154   \n",
       "\n",
       "      defaulted_in_last_6_months  \n",
       "755                            0  \n",
       "588                            0  \n",
       "585                            0  \n",
       "1329                           0  \n",
       "973                            0  \n",
       "\n",
       "[5 rows x 164 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# Prepare the ColumnTransformer\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), non_category_features)   # StandardScaler()\n",
    "    ],\n",
    "    remainder='passthrough'  # Leave categorical features untouched\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        feature_tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return feature_tensor, label_tensor\n",
    "    \n",
    "def custom_collate(batch):\n",
    "    divisor = len(DEVICE_LIST)  # Number of GPUs\n",
    "    batch_size = len(batch)\n",
    "    # print(\"batch shape \", batch)\n",
    "\n",
    "    # Calculate padding to make batch size divisible by the number of GPUs\n",
    "    pad_size = divisor - (batch_size % divisor) if batch_size % divisor != 0 else 0\n",
    "\n",
    "    # Separate features and labels\n",
    "    features = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "\n",
    "    # Pad features and labels along the batch dimension\n",
    "    if pad_size > 0:\n",
    "        pad_features = features[:pad_size]  # Reuse existing features for padding\n",
    "        pad_labels = labels[:pad_size]  # Reuse existing labels for padding\n",
    "        features = torch.cat([features, pad_features], dim=0)\n",
    "        labels = torch.cat([labels, pad_labels], dim=0)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUNING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    category_offsets: Optional[Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_numerical: int,\n",
    "        categories: Optional[List[int]],\n",
    "        d_token: int,\n",
    "        bias: bool,\n",
    "    ) -> None:\n",
    "        super(Tokenizer, self).__init__()\n",
    "        \n",
    "        # Handling categorical features\n",
    "        if categories is None:\n",
    "            d_bias = d_numerical\n",
    "            self.category_offsets = None\n",
    "            self.category_embeddings = None\n",
    "        else:\n",
    "            d_bias = d_numerical + len(categories)\n",
    "            \n",
    "            # Ensure proper indexing for embeddings\n",
    "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
    "            self.register_buffer('category_offsets', category_offsets)\n",
    "            \n",
    "            # Create category embeddings\n",
    "            self.category_embeddings = nn.Embedding(sum(categories), d_token)\n",
    "        \n",
    "        # Adding [CLS] token into account\n",
    "        self.weight = nn.Parameter(torch.empty(d_numerical + 1, d_token))\n",
    "        self.bias = nn.Parameter(torch.empty(d_bias, d_token)) if bias else None\n",
    "        \n",
    "        # The initialization is inspired by nn.Linear\n",
    "        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self) -> int:\n",
    "        return len(self.weight) + (0 if self.category_offsets is None else len(self.category_offsets))\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            x_num (Tensor): Numerical input tensor of shape (batch_size, num_features).\n",
    "            x_cat (Optional[Tensor]): Categorical input tensor of shape (batch_size, num_categories).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tokenized output of shape (batch_size, n_tokens, d_token).\n",
    "        \"\"\"\n",
    "        batch_size = x_num.size(0)\n",
    "        \n",
    "        # Add [CLS] token to numerical features\n",
    "        x_num = torch.cat([\n",
    "            torch.ones(batch_size, 1, device=x_num.device),  # [CLS] token\n",
    "            x_num\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Weight numerical features\n",
    "        x = self.weight.unsqueeze(0) * x_num.unsqueeze(-1)   # (1, d_num + 1, d_token) x (batch_size, d_num + 1, 1) => (batch_size, d_num + 1, d_token)\n",
    "        \n",
    "        # Handle categorical features if present\n",
    "        if x_cat is not None:\n",
    "            x_cat_embeddings = self.category_embeddings(x_cat + self.category_offsets.unsqueeze(0)) # (batch_size, num_categories, d_token)\n",
    "            x = torch.cat([x, x_cat_embeddings], dim=1) # batch_size, 1 + d_num + num_Categories, d_token\n",
    "        \n",
    "        # Add bias if applicable\n",
    "        if self.bias is not None:\n",
    "            bias = torch.cat([\n",
    "                torch.zeros(1, self.bias.size(1), device=x.device),\n",
    "                self.bias\n",
    "            ])\n",
    "            x = x + bias.unsqueeze(0)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d: int, n_heads: int, dropout: float = 0.0, kv_compression: Optional[float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Multi-head attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            d (int): Dimensionality of the input features.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout probability. Defaults to 0.0.\n",
    "            kv_compression: compression rate for key and value\n",
    "        \"\"\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        # Ensure d is divisible by n_heads\n",
    "        assert n_heads > 0 and d % n_heads == 0, \"d must be divisible by n_heads.\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d // n_heads\n",
    "\n",
    "        # Define linear transformations\n",
    "        self.W_q = nn.Linear(d, d)\n",
    "        self.W_k = nn.Linear(d, d)\n",
    "        self.W_v = nn.Linear(d, d)\n",
    "        self.W_out = nn.Linear(d, d) if n_heads > 1 else None\n",
    "        \n",
    "        self.kv_compression = kv_compression\n",
    "        if kv_compression:\n",
    "            self.key_compression = nn.Linear(d, int(d * kv_compression), bias=False)\n",
    "            self.value_compression = nn.Linear(d, int(d * kv_compression), bias=False)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "    def _reshape(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Reshape tensor for multi-head attention.\n",
    "\n",
    "        Input:\n",
    "            x: Tensor of shape (batch_size, n_tokens, d)\n",
    "\n",
    "        Output:\n",
    "            Tensor of shape (batch_size * n_heads, n_tokens, d_head)\n",
    "        \"\"\"\n",
    "        batch_size, n_tokens, d = x.shape\n",
    "        return (\n",
    "            x.reshape(batch_size, n_tokens, self.n_heads, self.d_head) \n",
    "            .transpose(1, 2)  # (batch_size, n_heads, n_tokens, d_head)\n",
    "            .reshape(batch_size * self.n_heads, n_tokens, self.d_head)  # (batch_size * n_heads, n_tokens, d_head)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_q: Tensor, x_kv: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            x_q (Tensor): Query tensor of shape (batch_size, num_tokens, d).\n",
    "            x_kv (Tensor): Key and value tensor of shape (batch_size, num_tokens, d).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, num_tokens, d).\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n",
    "        # q: (batch_size, n_q_tokens, d). k, v: (batch_size, num_tokens, d)\n",
    "        \n",
    "        # reduce dimension of key and value if it's too long\n",
    "        if self.kv_compression:\n",
    "            k = self.key_compression(k)\n",
    "            v = self.value_compression(v)\n",
    "\n",
    "        # Ensure dimensions are divisible by number of heads\n",
    "        for tensor in [q, k, v]:\n",
    "            assert tensor.shape[-1] % self.n_heads == 0, \"Input dimensions must be divisible by n_heads.\"\n",
    "\n",
    "        batch_size = q.size(0)\n",
    "        n_q_tokens = q.size(1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = self._reshape(q)  # (batch_size * n_heads, n_q_tokens, d_head)\n",
    "        k = self._reshape(k)  # (batch_size * n_heads, num_tokens, d_head)\n",
    "        v = self._reshape(v)  # (batch_size * n_heads, num_tokens, d_head)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_head)  # (batch_size * n_heads, n_q_tokens, num_tokens)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size * n_heads, n_q_tokens, num_tokens)\n",
    "\n",
    "        # Apply dropout to attention weights if specified\n",
    "        if self.dropout is not None:\n",
    "            attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Compute attention output\n",
    "        attention_output = attention_weights @ v  # (batch_size * n_heads, n_q_tokens, d_head)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        attention_output = (\n",
    "            attention_output\n",
    "            .reshape(batch_size, self.n_heads, n_q_tokens, self.d_head)  # (batch_size, n_heads, n_q_tokens, d_head)\n",
    "            .transpose(1, 2)  # (batch_size, n_q_tokens, n_heads, d_head)\n",
    "            .reshape(batch_size, n_q_tokens, -1)  # (batch_size, n_q_tokens, d)\n",
    "        )\n",
    "\n",
    "        # Apply final linear transformation if applicable\n",
    "        if self.W_out is not None:\n",
    "            attention_output = self.W_out(attention_output)  # (batch_size, n_q_tokens, d)\n",
    "\n",
    "        return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reglu(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"ReGLU activation function.\"\"\"\n",
    "    a, b = x.chunk(2, dim=-1)\n",
    "    return a * F.relu(b)\n",
    "\n",
    "def geglu(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"GeGLU activation function.\"\"\"\n",
    "    a, b = x.chunk(2, dim=-1)\n",
    "    return a * F.gelu(b)\n",
    "\n",
    "def get_activation_fn(name: str) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Retrieve the activation function by name.\"\"\"\n",
    "    activation_functions = {\n",
    "        'reglu': reglu,\n",
    "        'geglu': geglu,\n",
    "        'sigmoid': torch.sigmoid,\n",
    "    }\n",
    "\n",
    "    if name in activation_functions:\n",
    "        return activation_functions[name]\n",
    "\n",
    "    if hasattr(F, name):\n",
    "        return getattr(F, name)\n",
    "\n",
    "    raise ValueError(f\"Unsupported activation function: {name}\")\n",
    "\n",
    "def get_nonglu_activation_fn(name: str) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Retrieve the non-GLU activation function by name.\"\"\"\n",
    "    nonglu_mappings = {\n",
    "        'reglu': F.relu,\n",
    "        'geglu': F.gelu,\n",
    "    }\n",
    "\n",
    "    return nonglu_mappings.get(name, get_activation_fn(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_token: int,\n",
    "        d_hidden: int,\n",
    "        n_heads: int,\n",
    "        attention_dropout: float,\n",
    "        activation: str,\n",
    "        prenormalization: bool,\n",
    "        kv_compression: Optional[float],\n",
    "        ffn_dropout: Optional[float],\n",
    "        residual_dropout: Optional[float]) -> None:\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.norm0 = nn.LayerNorm(d_token) if not prenormalization else nn.Identity()\n",
    "        self.attention = MultiheadAttention(d_token, n_heads, attention_dropout, kv_compression)\n",
    "        self.linear0 = nn.Linear(d_token, d_hidden * (2 if activation.endswith('glu') else 1))\n",
    "    \n",
    "        self.dropout0 = nn.Dropout(residual_dropout) if residual_dropout else nn.Identity()\n",
    "        self.linear1 = nn.Linear(d_hidden, d_token)\n",
    "        self.norm1 = nn.LayerNorm(d_token) if not prenormalization else nn.Identity()\n",
    "        self.dropout1 = nn.Dropout(ffn_dropout) if ffn_dropout else nn.Identity()\n",
    "        self.activation = get_activation_fn(activation)\n",
    "        \n",
    "    def forward(self, x, is_last_layer=False):\n",
    "        # attention block\n",
    "        residual = self.norm0(x)\n",
    "        residual = self.attention(residual[:, :1]if is_last_layer else residual, #[CLS] - target variable,\n",
    "                                  residual) \n",
    "        residual = self.dropout0(residual) + x\n",
    "        residual = self.norm1(residual)\n",
    "        x = residual\n",
    "        \n",
    "        # feedforward block\n",
    "        residual = self.norm0(residual)\n",
    "        residual = self.linear0(residual)\n",
    "        residual = self.activation(residual)\n",
    "        residual = self.dropout1(residual)\n",
    "        residual = self.linear1(residual)\n",
    "        residual = self.dropout0(residual) + x\n",
    "        residual = self.norm1(residual)\n",
    "        return residual\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer implementation with support for optional Linformer compression.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_numerical: int,\n",
    "        categories: Optional[List[int]],\n",
    "        token_bias: bool,\n",
    "        n_layers: int,\n",
    "        d_token: int,\n",
    "        n_heads: int,\n",
    "        d_ffn_factor: float,\n",
    "        attention_dropout: float,\n",
    "        ffn_dropout: Optional[float],\n",
    "        residual_dropout: Optional[float],\n",
    "        activation: str,\n",
    "        prenormalization: bool,\n",
    "        kv_compression: Optional[float],\n",
    "        d_out: int,\n",
    "    ) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.tokenizer = Tokenizer(d_numerical, categories, d_token, token_bias)\n",
    "        d_hidden = int(d_token * d_ffn_factor)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_token, d_hidden, n_heads, attention_dropout, activation, prenormalization, kv_compression, ffn_dropout, residual_dropout) for _ in range(n_layers)])\n",
    "\n",
    "        \n",
    "        # use non-glu activation function in the last layer for simplicity\n",
    "        self.last_activation = get_nonglu_activation_fn(activation)\n",
    "        self.prenormalization = prenormalization\n",
    "        self.last_normalization = nn.LayerNorm(d_token) if prenormalization else None\n",
    "        self.head = nn.Linear(d_token, d_out)\n",
    "\n",
    "    def forward(self, x_num: torch.Tensor, x_cat: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Transformer model.\"\"\"\n",
    "        x = self.tokenizer(x_num, x_cat)\n",
    "        x_residual = x\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            is_last_layer = layer_idx + 1 == len(self.layers)\n",
    "            x_residual = layer(x_residual, is_last_layer)\n",
    "\n",
    "        # Final normalization and activation\n",
    "        x_residual = x_residual[:, 0]\n",
    "        if self.last_normalization is not None:\n",
    "            x_residual = self.last_normalization(x_residual)\n",
    "        x_residual = self.last_activation(x_residual)\n",
    "        output = self.head(x_residual)\n",
    "        # print(\"dimension of ori output: \", output.shape)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    # model = define_model(trial, train_features.shape[1], 1).to(DEVICE)\n",
    "    # Define out_features_list\n",
    "    n_heads = trial.suggest_int(\"n_heads\", 1, 10)\n",
    "    \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    \n",
    "    token_multiplier = trial.suggest_int(\"token_multiplier\", 5, 30)  # Adjust the range as necessary\n",
    "    \n",
    "    # Suggest an integer for a that is divisible by b\n",
    "    d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, step=n_heads)\n",
    "    \n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0, 0.5)\n",
    "    d_ffn_factor = trial.suggest_float(\"d_ffn_factor\", 1, 3)\n",
    "    ffn_dropout = trial.suggest_float(\"ffn_dropout\", 0, 0.5)\n",
    "    \n",
    "    # activation = trial.suggest_categorical(\"activation\", choices=['relu', 'reglu', 'geglu'])\n",
    "    # batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
    "\n",
    "    args = {'activation': 'relu', #activation, #\n",
    "    'attention_dropout': attention_dropout,\n",
    "    'd_ffn_factor': d_ffn_factor,\n",
    "    'd_token': d_token,\n",
    "    'ffn_dropout': ffn_dropout,\n",
    "    'n_heads': n_heads,\n",
    "    'n_layers': n_layers,\n",
    "    'prenormalization': False,\n",
    "    'residual_dropout': 0.0,\n",
    "    'kv_compression': None,\n",
    "    'token_bias': True,\n",
    "    'd_out': 1\n",
    "    }\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-9, 1e-1, log=True)\n",
    "\n",
    "    # training with 5-fold CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_features):\n",
    "        # Create training and validation datasets for the current fold\n",
    "        X_train_fold, X_val_fold = train_features.iloc[train_idx], train_features.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = train_labels.iloc[train_idx], train_labels.iloc[val_idx]\n",
    "        \n",
    "        # scaling features\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "            \n",
    "        # Initialize the model for this fold\n",
    "        model = Transformer(d_numerical=d_numerical, categories=categories, **args).to(DEVICE)\n",
    "        model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # define optimizer\n",
    "        if optimizer_name == \"Adam\":\n",
    "         optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            momentum = trial.suggest_float(\"momentum\", 1e-9, 0.95, log=True)\n",
    "            optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        \n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Prepare DataLoader for training\n",
    "        train_dataset = CustomDataset(X_train_fold, y_train_fold.to_numpy())\n",
    "        val_dataset = CustomDataset(X_val_fold, y_val_fold.to_numpy())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "            \n",
    "        # Training of the model.\n",
    "        model.train()\n",
    "        for epoch in range(EPOCHS):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                \n",
    "                X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "                X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # print(\"shape\", X_num.shape, X_cat.shape)\n",
    "                output = model(X_num, X_cat)\n",
    "\n",
    "                # print(\"shape of target var\", output.shape, target.shape)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "                X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "                output = model(X_num, X_cat)\n",
    "                \n",
    "                # print(\"shape of val var: \", output.shape, target.shape)\n",
    "                val_loss = criterion(output, target).item()\n",
    "                val_losses.append(val_loss**0.5) #rmse\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "    # Return the average validation loss across all folds\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-11 15:00:50,008] A new study created in memory with name: no-name-243854ea-9794-4964-b34a-e8d386968e3b\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-12-11 15:04:03,619] Trial 0 finished with value: 0.26834642141950416 and parameters: {'n_heads': 6, 'n_layers': 4, 'token_multiplier': 14, 'd_token': 54, 'attention_dropout': 0.44210847022425526, 'd_ffn_factor': 1.8304837732415857, 'ffn_dropout': 0.4694615865409912, 'optimizer': 'Adam', 'lr': 0.0010108795198581204, 'weight_decay': 0.01618203056711717}. Best is trial 0 with value: 0.26834642141950416.\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-12-11 15:07:50,172] Trial 1 finished with value: 0.3189764472797721 and parameters: {'n_heads': 6, 'n_layers': 5, 'token_multiplier': 17, 'd_token': 6, 'attention_dropout': 0.016380208557858655, 'd_ffn_factor': 2.821030393983747, 'ffn_dropout': 0.35069134519487793, 'optimizer': 'Adam', 'lr': 5.440245251013737e-05, 'weight_decay': 0.005349542900693531}. Best is trial 0 with value: 0.26834642141950416.\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-12-11 15:09:06,011] Trial 2 finished with value: 0.3232038275764762 and parameters: {'n_heads': 3, 'n_layers': 1, 'token_multiplier': 12, 'd_token': 15, 'attention_dropout': 0.45131139773990386, 'd_ffn_factor': 1.6057219373302385, 'ffn_dropout': 0.3957764532470824, 'optimizer': 'RMSprop', 'lr': 0.04829799893383766, 'weight_decay': 1.2830019993131303e-07, 'momentum': 0.0001664214126595555}. Best is trial 0 with value: 0.26834642141950416.\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-12-11 15:12:10,554] Trial 3 finished with value: 0.2579254291284334 and parameters: {'n_heads': 8, 'n_layers': 4, 'token_multiplier': 25, 'd_token': 88, 'attention_dropout': 0.07472463283355035, 'd_ffn_factor': 1.3200781582994598, 'ffn_dropout': 0.057767167109364026, 'optimizer': 'Adam', 'lr': 0.0012173762540881236, 'weight_decay': 0.001057582767063431}. Best is trial 3 with value: 0.2579254291284334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  4\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  4\n",
      "Best trial:\n",
      "  Value:  0.2579254291284334\n",
      "  Params: \n",
      "    n_heads: 8\n",
      "    n_layers: 4\n",
      "    token_multiplier: 25\n",
      "    d_token: 88\n",
      "    attention_dropout: 0.07472463283355035\n",
      "    d_ffn_factor: 1.3200781582994598\n",
      "    ffn_dropout: 0.057767167109364026\n",
      "    optimizer: Adam\n",
      "    lr: 0.0012173762540881236\n",
      "    weight_decay: 0.001057582767063431\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_heads': 8,\n",
       " 'n_layers': 4,\n",
       " 'token_multiplier': 25,\n",
       " 'd_token': 88,\n",
       " 'attention_dropout': 0.07472463283355035,\n",
       " 'd_ffn_factor': 1.3200781582994598,\n",
       " 'ffn_dropout': 0.057767167109364026,\n",
       " 'optimizer': 'Adam',\n",
       " 'lr': 0.0012173762540881236,\n",
       " 'weight_decay': 0.001057582767063431}"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\"model\": {}, \"optimizer\": {}}\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    if key in ['lr', 'weight_decay', 'momentum', 'weight_decay', 'optimizer']:\n",
    "        MODEL_CONFIG[\"optimizer\"][key] = value\n",
    "    elif key == 'token_multiplier':\n",
    "        continue\n",
    "    elif key == 'batch_size':\n",
    "        BATCH_SIZE = value\n",
    "    else:\n",
    "        # adj_key = key.rpartition('_')[0]\n",
    "        MODEL_CONFIG[\"model\"][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'n_heads': 8,\n",
       "  'n_layers': 4,\n",
       "  'd_token': 88,\n",
       "  'attention_dropout': 0.07472463283355035,\n",
       "  'd_ffn_factor': 1.3200781582994598,\n",
       "  'ffn_dropout': 0.057767167109364026},\n",
       " 'optimizer': {'optimizer': 'Adam',\n",
       "  'lr': 0.0012173762540881236,\n",
       "  'weight_decay': 0.001057582767063431}}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "new_feature_list = non_category_features + category_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = CustomDataset(train_features, train_labels.to_numpy())\n",
    "test_dataset = CustomDataset(test_features, test_labels.to_numpy())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_features.shape[0], shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [len(le.classes_) for le in label_encoders.values()]\n",
    "d_numerical = len(non_category_features)\n",
    "# d_token = 32  # Example token dimension\n",
    "# token_bias = True\n",
    "\n",
    "args = {\n",
    "  # 'initialization': 'kaiming',\n",
    "  'activation': 'relu',\n",
    "    'prenormalization': False,\n",
    "    'residual_dropout': 0.0,\n",
    "    'kv_compression': None,\n",
    "    'token_bias': True,\n",
    "    'd_out': 1\n",
    "}\n",
    "\n",
    "args.update(MODEL_CONFIG[\"model\"])\n",
    "model = Transformer(d_numerical=d_numerical, categories=categories, **args).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tokenizer): Tokenizer(\n",
       "    (category_embeddings): Embedding(166, 88)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (norm0): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=88, out_features=88, bias=True)\n",
       "        (W_k): Linear(in_features=88, out_features=88, bias=True)\n",
       "        (W_v): Linear(in_features=88, out_features=88, bias=True)\n",
       "        (W_out): Linear(in_features=88, out_features=88, bias=True)\n",
       "        (dropout): Dropout(p=0.07472463283355035, inplace=False)\n",
       "      )\n",
       "      (linear0): Linear(in_features=88, out_features=116, bias=True)\n",
       "      (dropout0): Identity()\n",
       "      (linear1): Linear(in_features=116, out_features=88, bias=True)\n",
       "      (norm1): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.057767167109364026, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=88, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Transformer(\n",
       "    (tokenizer): Tokenizer(\n",
       "      (category_embeddings): Embedding(166, 88)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerBlock(\n",
       "        (norm0): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiheadAttention(\n",
       "          (W_q): Linear(in_features=88, out_features=88, bias=True)\n",
       "          (W_k): Linear(in_features=88, out_features=88, bias=True)\n",
       "          (W_v): Linear(in_features=88, out_features=88, bias=True)\n",
       "          (W_out): Linear(in_features=88, out_features=88, bias=True)\n",
       "          (dropout): Dropout(p=0.07472463283355035, inplace=False)\n",
       "        )\n",
       "        (linear0): Linear(in_features=88, out_features=116, bias=True)\n",
       "        (dropout0): Identity()\n",
       "        (linear1): Linear(in_features=116, out_features=88, bias=True)\n",
       "        (norm1): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.057767167109364026, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (head): Linear(in_features=88, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE != \"cpu\":\n",
    "    model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0012173762540881236\n",
       "    maximize: False\n",
       "    weight_decay: 0.001057582767063431\n",
       ")"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define optimizer\n",
    "optim_config = deepcopy(MODEL_CONFIG[\"optimizer\"])\n",
    "del optim_config[\"optimizer\"]\n",
    "\n",
    "optimizer = getattr(optim, MODEL_CONFIG[\"optimizer\"][\"optimizer\"])(model.parameters(), **optim_config)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7efebc3e4e60>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/500 [00:46<06:57,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51], Train Loss: 0.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 151/500 [02:18<05:43,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151], Train Loss: 0.1806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 251/500 [03:51<03:49,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [251], Train Loss: 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 351/500 [05:22<02:16,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351], Train Loss: 0.1669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 451/500 [06:53<00:45,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451], Train Loss: 0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:38<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 458.319 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 500\n",
    "criterion = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "train_errors = []\n",
    "\n",
    "for ep in tqdm(range(EPOCH)):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "        X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X_num, X_cat)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    train_loss = running_loss  / len(train_loader)\n",
    "    train_errors.append(train_loss**0.5)\n",
    "    if ep % 100 == 50:\n",
    "        print(f'Epoch [{ep+1}], Train Loss: {train_loss**0.5:.4f}')\n",
    "    \n",
    "# train_loss = running_loss  / len(train_loader)\n",
    "# print(f'Epoch [{ep+1}], Train Loss: {train_loss**0.5:.4f}')\n",
    "\n",
    "# print out training time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training time: {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training average mean absolute error: 0.1805223524570465\n",
      "Training average mean absolute percentage error: 223.90522956848145\n",
      "Training average root mean squared error: 0.25800502697627115\n",
      "Training average R2: 0.4188556671142578\n"
     ]
    }
   ],
   "source": [
    "# Testing phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for data, target in test_loader:\n",
    "        # inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        X_num = data[:, :len(non_category_features)]\n",
    "        X_cat = data[:, -len(category_features):].detach().long()\n",
    "        \n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model = model.module  # Unwrap from DataParallel\n",
    "        model = model.to('cpu')\n",
    "            \n",
    "               \n",
    "        outputs = model(X_num, X_cat)\n",
    "\n",
    "        # save metrics\n",
    "        mae, mape, rmse, rsqr = calculate_metric(outputs.numpy(), target.numpy())\n",
    "        print(f\"Test average mean absolute error: {mae}\")\n",
    "        print(f\"Test average mean absolute percentage error: {mape}\")\n",
    "        print(f\"Test average root mean squared error: {rmse}\")\n",
    "        print(f\"Test average R2: {rsqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s load back in our saved model\n",
    "# model = MLP()\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 0 batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.2570\n",
      "Epoch [500], Train Loss: 0.1957\n",
      "Epoch [500], Train Loss: 0.1786\n",
      "Epoch [500], Train Loss: 0.1760\n",
      "Epoch [500], Train Loss: 0.1694\n",
      "Start 1 batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.2653\n",
      "Epoch [500], Train Loss: 0.1995\n",
      "Epoch [500], Train Loss: 0.1822\n",
      "Epoch [500], Train Loss: 0.1752\n",
      "Epoch [500], Train Loss: 0.1739\n",
      "Start 2 batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.2657\n",
      "Epoch [500], Train Loss: 0.2078\n",
      "Epoch [500], Train Loss: 0.2011\n",
      "Epoch [500], Train Loss: 0.1982\n",
      "Epoch [500], Train Loss: 0.2151\n",
      "Start 3 batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.2584\n",
      "Epoch [500], Train Loss: 0.2099\n",
      "Epoch [500], Train Loss: 0.1923\n",
      "Epoch [500], Train Loss: 0.1891\n",
      "Epoch [500], Train Loss: 0.1821\n",
      "Start 4 batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.2615\n",
      "Epoch [500], Train Loss: 0.1791\n",
      "Epoch [500], Train Loss: 0.1794\n",
      "Epoch [500], Train Loss: 0.1663\n",
      "Epoch [500], Train Loss: 0.1691\n"
     ]
    }
   ],
   "source": [
    "# Define cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "EPOCHS = 500\n",
    "val_mae = []\n",
    "val_mape = []\n",
    "val_rmse = []\n",
    "val_rsqr = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for train_idx, val_idx in kf.split(features):\n",
    "    running_loss = 0\n",
    "    train_errors = []\n",
    "\n",
    "    print(f\"Start {i} batch\")\n",
    "    i += 1\n",
    "    \n",
    "    # Create training and validation datasets for the current fold\n",
    "    X_train_fold, X_val_fold = features.iloc[train_idx], features.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = labels.iloc[train_idx], labels.iloc[val_idx]\n",
    "    \n",
    "    # scaling features\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "        \n",
    "    # Initialize the model for this fold\n",
    "    model = Transformer(d_numerical=d_numerical, categories=categories, **args)\n",
    "    model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = getattr(optim, MODEL_CONFIG[\"optimizer\"][\"optimizer\"])(model.parameters(), **optim_config)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Prepare DataLoader for training\n",
    "    train_dataset = CustomDataset(X_train_fold, y_train_fold.to_numpy())\n",
    "    val_dataset = CustomDataset(X_val_fold, y_val_fold.to_numpy())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_dataset.features.shape[0], shuffle=True)\n",
    "        \n",
    "    # Training of the model.\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "  \n",
    "            X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "            X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(X_num, X_cat)\n",
    "\n",
    "            \n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        train_loss = running_loss  / len(train_loader)\n",
    "        train_errors.append(train_loss**0.5)\n",
    "        if epoch % 100 == 20:\n",
    "            print(f'Epoch [{ep+1}], Train Loss: {train_loss**0.5:.4f}')\n",
    "            \n",
    "    # save model state_dict\n",
    "    torch.save(model.state_dict(),  f\"{MODEL_PATH}/transformer_weights_fold_{i}.pth\")\n",
    "\n",
    "    # Validation of the model.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            X_num = data[:, :len(non_category_features)]\n",
    "            X_cat = data[:, -len(category_features):].detach().long()\n",
    "            \n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                model = model.module  # Unwrap from DataParallel\n",
    "            model = model.to('cpu')\n",
    "            outputs = model(X_num, X_cat)\n",
    "            \n",
    "            # save metrics\n",
    "            mae, mape, rmse, rsqr = calculate_metric(outputs.numpy(), target.numpy())\n",
    "            val_mae.append(mae)\n",
    "            val_mape.append(mape)\n",
    "            val_rmse.append(rmse)\n",
    "            val_rsqr.append(rsqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25521199392431193,\n",
       " 0.28332516901557664,\n",
       " 0.23299730218043802,\n",
       " 0.2331523955102855,\n",
       " 0.2667226411639532]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average mean absolute error: 0.17922967672348022\n",
      "Test average mean absolute percentage error: 6145.480759143829\n",
      "Test average root mean squared error: 0.25428190035891307\n",
      "Test average R2: 0.403901481628418\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test average mean absolute error: {statistics.mean(val_mae)}\")\n",
    "print(f\"Test average mean absolute percentage error: {statistics.mean(val_mape)}\")\n",
    "print(f\"Test average root mean squared error: {statistics.mean(val_rmse)}\")\n",
    "print(f\"Test average R2: {statistics.mean(val_rsqr)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".henv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
