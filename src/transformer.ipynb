{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 sympy-1.13.1 torch-2.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Callable, cast, Dict\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# import typing as ty\n",
    "from torch import Tensor\n",
    "import torch.nn.init as nn_init\n",
    "import statistics\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from utils import calculate_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"../data/all_data_cleaned.xlsx\"\n",
    "MODEL_PATH = '../output/transformer/best_transformer_model.pth'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = 'cpu'\n",
    "DEVICE_LIST = [0, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rr1_7</th>\n",
       "      <th>rr2_7</th>\n",
       "      <th>rr1_30</th>\n",
       "      <th>rr2_30</th>\n",
       "      <th>currency</th>\n",
       "      <th>seniorioty</th>\n",
       "      <th>seniorioty_adj</th>\n",
       "      <th>coupon rate</th>\n",
       "      <th>coupon frequency</th>\n",
       "      <th>maturity_type</th>\n",
       "      <th>...</th>\n",
       "      <th>PD_56_pd</th>\n",
       "      <th>PD_57_pd</th>\n",
       "      <th>PD_58_pd</th>\n",
       "      <th>PD_59_pd</th>\n",
       "      <th>PD_60_pd</th>\n",
       "      <th>DTD</th>\n",
       "      <th>NI_Over_TA</th>\n",
       "      <th>Size</th>\n",
       "      <th>defaulted_in_last_6_months</th>\n",
       "      <th>default_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.241969</td>\n",
       "      <td>0.259908</td>\n",
       "      <td>0.252843</td>\n",
       "      <td>USD</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>9.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CALL/SINK</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397453</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.398819</td>\n",
       "      <td>0.399467</td>\n",
       "      <td>0.400092</td>\n",
       "      <td>-0.732815</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.852484</td>\n",
       "      <td>False</td>\n",
       "      <td>6.073973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030148</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.032729</td>\n",
       "      <td>0.031998</td>\n",
       "      <td>USD</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>5.750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CONV/CALL</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.957480</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.957503</td>\n",
       "      <td>0.957514</td>\n",
       "      <td>-1.666262</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-1.186347</td>\n",
       "      <td>False</td>\n",
       "      <td>5.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.969841</td>\n",
       "      <td>0.960390</td>\n",
       "      <td>0.972400</td>\n",
       "      <td>0.960490</td>\n",
       "      <td>USD</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>5.675</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PUTABLE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>False</td>\n",
       "      <td>3.145205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.047361</td>\n",
       "      <td>1.046199</td>\n",
       "      <td>1.047416</td>\n",
       "      <td>1.046196</td>\n",
       "      <td>CHF</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CONVERTIBLE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>False</td>\n",
       "      <td>3.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.848102</td>\n",
       "      <td>0.840452</td>\n",
       "      <td>0.848872</td>\n",
       "      <td>0.840574</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CONV/CALL</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130688</td>\n",
       "      <td>0.131081</td>\n",
       "      <td>0.131465</td>\n",
       "      <td>0.131840</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>-0.768857</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-1.946507</td>\n",
       "      <td>False</td>\n",
       "      <td>7.153425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rr1_7     rr2_7    rr1_30    rr2_30 currency  \\\n",
       "0  0.247786  0.241969  0.259908  0.252843      USD   \n",
       "1  0.030148  0.029552  0.032729  0.031998      USD   \n",
       "2  0.969841  0.960390  0.972400  0.960490      USD   \n",
       "3  1.047361  1.046199  1.047416  1.046196      CHF   \n",
       "4  0.848102  0.840452  0.848872  0.840574      JPY   \n",
       "\n",
       "                      seniorioty                 seniorioty_adj  coupon rate  \\\n",
       "0  Senior Subordinated Unsecured  Senior Subordinated Unsecured        9.000   \n",
       "1  Senior Subordinated Unsecured  Senior Subordinated Unsecured        5.750   \n",
       "2                      Unsecured                      Unsecured        5.675   \n",
       "3                      Unsecured                      Unsecured        0.125   \n",
       "4                      Unsecured                      Unsecured        1.750   \n",
       "\n",
       "   coupon frequency maturity_type  ...  PD_56_pd  PD_57_pd  PD_58_pd  \\\n",
       "0               2.0     CALL/SINK  ...  0.397453  0.398148  0.398819   \n",
       "1               2.0     CONV/CALL  ...  0.957467  0.957480  0.957492   \n",
       "2               2.0       PUTABLE  ...  0.568693  0.569197  0.569682   \n",
       "3               1.0   CONVERTIBLE  ...  0.568693  0.569197  0.569682   \n",
       "4               1.0     CONV/CALL  ...  0.130688  0.131081  0.131465   \n",
       "\n",
       "   PD_59_pd  PD_60_pd       DTD NI_Over_TA      Size  \\\n",
       "0  0.399467  0.400092 -0.732815  -0.007137 -0.852484   \n",
       "1  0.957503  0.957514 -1.666262  -0.000286 -1.186347   \n",
       "2  0.570150  0.570600 -1.853366   0.000191  1.053677   \n",
       "3  0.570150  0.570600 -1.853366   0.000191  1.053677   \n",
       "4  0.131840  0.132206 -0.768857  -0.028058 -1.946507   \n",
       "\n",
       "  defaulted_in_last_6_months default_duration  \n",
       "0                      False         6.073973  \n",
       "1                      False         5.109589  \n",
       "2                      False         3.145205  \n",
       "3                      False         3.002740  \n",
       "4                      False         7.153425  \n",
       "\n",
       "[5 rows x 179 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 179)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['rr1_30']\n",
    "features = df.drop(columns=['rr1_30', 'rr1_7', 'rr2_7', 'rr2_30'], errors=\"ignore\") #drop labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currency                       object\n",
       "seniorioty                     object\n",
       "seniorioty_adj                 object\n",
       "coupon rate                   float64\n",
       "coupon frequency              float64\n",
       "                               ...   \n",
       "DTD                           float64\n",
       "NI_Over_TA                    float64\n",
       "Size                          float64\n",
       "defaulted_in_last_6_months       bool\n",
       "default_duration              float64\n",
       "Length: 175, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = features.columns\n",
    "category_features = list(features.select_dtypes(include=['object', 'bool']).columns)\n",
    "non_category_features = [i for i in feature_list if i not in category_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(non_category_features))\n",
    "print(len(category_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce all categories to have type string\n",
    "features[category_features] = features[category_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical features in features\n",
    "label_encoders = {}\n",
    "mappings = {}\n",
    "\n",
    "for column in category_features:\n",
    "    le = LabelEncoder()\n",
    "    features[column] = le.fit_transform(features[column])\n",
    "    label_encoders[column] = le\n",
    "    mappings[column] = {index: label for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currency': {0: 'CAD',\n",
       "  1: 'CHF',\n",
       "  2: 'CNY',\n",
       "  3: 'EUR',\n",
       "  4: 'GBP',\n",
       "  5: 'HKD',\n",
       "  6: 'INR',\n",
       "  7: 'ISK',\n",
       "  8: 'JPY',\n",
       "  9: 'MYR',\n",
       "  10: 'NOK',\n",
       "  11: 'SEK',\n",
       "  12: 'SGD',\n",
       "  13: 'THB',\n",
       "  14: 'TWD',\n",
       "  15: 'USD'},\n",
       " 'seniorioty': {0: 'Junior Unsecured or Junior Subordinated Unsecured',\n",
       "  1: 'Secured',\n",
       "  2: 'Senior Secured',\n",
       "  3: 'Senior Secured - First Lien',\n",
       "  4: 'Senior Secured - First Mortgage',\n",
       "  5: 'Senior Secured - Mortgage',\n",
       "  6: 'Senior Secured - Second Lien',\n",
       "  7: 'Senior Subordinated Unsecured',\n",
       "  8: 'Senior Unsecured',\n",
       "  9: 'Subordinated Unsecured',\n",
       "  10: 'Unsecured'},\n",
       " 'seniorioty_adj': {0: 'Junior Unsecured or Junior Subordinated Unsecured',\n",
       "  1: 'Secured',\n",
       "  2: 'Senior Secured',\n",
       "  3: 'Senior Subordinated Unsecured',\n",
       "  4: 'Senior Unsecured',\n",
       "  5: 'Subordinated Unsecured',\n",
       "  6: 'Unsecured'},\n",
       " 'maturity_type': {0: 'AT MATURITY',\n",
       "  1: 'CALL/PUT',\n",
       "  2: 'CALL/REF/SINK',\n",
       "  3: 'CALL/SINK',\n",
       "  4: 'CALL/SINK/PUT',\n",
       "  5: 'CALLABLE',\n",
       "  6: 'CONV/CALL',\n",
       "  7: 'CONV/CALL/SINK',\n",
       "  8: 'CONV/PUT',\n",
       "  9: 'CONV/PUT/CALL',\n",
       "  10: 'CONVERTIBLE',\n",
       "  11: 'PUTABLE',\n",
       "  12: 'SINK/PUT',\n",
       "  13: 'SINKABLE',\n",
       "  14: 'nan'},\n",
       " 'call type': {0: 'Make Whole Call',\n",
       "  1: 'Ordinary Call',\n",
       "  2: 'Special Call',\n",
       "  3: 'nan'},\n",
       " 'put type': {0: 'On Specified Dates', 1: 'On or After Any Date', 2: 'nan'},\n",
       " 'Convertible': {0: 'Convertible', 1: 'nan'},\n",
       " 'coupon type -code': {0: 'FRFF',\n",
       "  1: 'FRFX',\n",
       "  2: 'FROT',\n",
       "  3: 'FRPV',\n",
       "  4: 'FTZR',\n",
       "  5: 'FXDI',\n",
       "  6: 'FXPM',\n",
       "  7: 'FXPV',\n",
       "  8: 'FXZC',\n",
       "  9: 'VRGR',\n",
       "  10: 'ZRFX',\n",
       "  11: 'ZRVR'},\n",
       " 'domicile_country': {0: 'Argentina',\n",
       "  1: 'Australia',\n",
       "  2: 'Bahamas',\n",
       "  3: 'Belgium',\n",
       "  4: 'Bermuda',\n",
       "  5: 'Canada',\n",
       "  6: 'Cayman Islands',\n",
       "  7: 'China',\n",
       "  8: 'Czech Republic',\n",
       "  9: 'Greece',\n",
       "  10: 'Hong Kong',\n",
       "  11: 'Iceland',\n",
       "  12: 'India',\n",
       "  13: 'Indonesia',\n",
       "  14: 'Japan',\n",
       "  15: 'Luxembourg',\n",
       "  16: 'Malaysia',\n",
       "  17: 'Mongolia',\n",
       "  18: 'Philippines',\n",
       "  19: 'Poland',\n",
       "  20: 'Singapore',\n",
       "  21: 'South Africa',\n",
       "  22: 'South Korea',\n",
       "  23: 'Taiwan',\n",
       "  24: 'Thailand',\n",
       "  25: 'United Kingdom',\n",
       "  26: 'United States'},\n",
       " 'exchange_country': {0: 'Australia',\n",
       "  1: 'China',\n",
       "  2: 'Hong Kong',\n",
       "  3: 'India',\n",
       "  4: 'Indonesia',\n",
       "  5: 'Japan',\n",
       "  6: 'Malaysia',\n",
       "  7: 'Philippines',\n",
       "  8: 'Singapore',\n",
       "  9: 'South Korea',\n",
       "  10: 'Taiwan',\n",
       "  11: 'Thailand',\n",
       "  12: 'United States'},\n",
       " 'Industry_sector': {0: 'Communications',\n",
       "  1: 'Consumer Discretionary',\n",
       "  2: 'Consumer Staples',\n",
       "  3: 'Energy',\n",
       "  4: 'Financials',\n",
       "  5: 'Health Care',\n",
       "  6: 'Industrials',\n",
       "  7: 'Materials',\n",
       "  8: 'Real Estate',\n",
       "  9: 'Technology',\n",
       "  10: 'Utilities'},\n",
       " 'Industry_group': {0: 'Banking',\n",
       "  1: 'Consumer Discretionary Products',\n",
       "  2: 'Consumer Discretionary Services',\n",
       "  3: 'Consumer Staple Products',\n",
       "  4: 'Health Care',\n",
       "  5: 'Industrial Products',\n",
       "  6: 'Industrial Services',\n",
       "  7: 'Insurance',\n",
       "  8: 'Materials',\n",
       "  9: 'Media',\n",
       "  10: 'Oil & Gas',\n",
       "  11: 'Real Estate',\n",
       "  12: 'Renewable Energy',\n",
       "  13: 'Retail & Wholesale - Staples',\n",
       "  14: 'Retail & Whsle - Discretionary',\n",
       "  15: 'Software & Tech Services',\n",
       "  16: 'Tech Hardware & Semiconductors',\n",
       "  17: 'Telecommunications',\n",
       "  18: 'Utilities'},\n",
       " 'Industry_subgroup': {0: 'Advertising & Marketing',\n",
       "  1: 'Apparel & Textile Products',\n",
       "  2: 'Automotive',\n",
       "  3: 'Banking',\n",
       "  4: 'Beverages',\n",
       "  5: 'Biotech & Pharma',\n",
       "  6: 'Cable & Satellite',\n",
       "  7: 'Chemicals',\n",
       "  8: 'Commercial Support Services',\n",
       "  9: 'Construction Materials',\n",
       "  10: 'Consumer Services',\n",
       "  11: 'Containers & Packaging',\n",
       "  12: 'E-Commerce Discretionary',\n",
       "  13: 'Electric Utilities',\n",
       "  14: 'Electrical Equipment',\n",
       "  15: 'Engineering & Construction',\n",
       "  16: 'Entertainment Content',\n",
       "  17: 'Food',\n",
       "  18: 'Forestry, Paper & Wood Products',\n",
       "  19: 'Gas & Water Utilities',\n",
       "  20: 'Health Care Facilities & Svcs',\n",
       "  21: 'Home & Office Products',\n",
       "  22: 'Home Construction',\n",
       "  23: 'Household Products',\n",
       "  24: 'Industrial Intermediate Prod',\n",
       "  25: 'Industrial Support Services',\n",
       "  26: 'Insurance',\n",
       "  27: 'Leisure Facilities & Services',\n",
       "  28: 'Leisure Products',\n",
       "  29: 'Machinery',\n",
       "  30: 'Medical Equipment & Devices',\n",
       "  31: 'Metals & Mining',\n",
       "  32: 'Oil & Gas Producers',\n",
       "  33: 'Oil & Gas Services & Equip',\n",
       "  34: 'Publishing & Broadcasting',\n",
       "  35: 'REIT',\n",
       "  36: 'Real Estate Owners & Developers',\n",
       "  37: 'Real Estate Services',\n",
       "  38: 'Renewable Energy',\n",
       "  39: 'Retail - Consumer Staples',\n",
       "  40: 'Retail - Discretionary',\n",
       "  41: 'Semiconductors',\n",
       "  42: 'Software',\n",
       "  43: 'Steel',\n",
       "  44: 'Technology Hardware',\n",
       "  45: 'Technology Services',\n",
       "  46: 'Telecommunications',\n",
       "  47: 'Transportation & Logistics',\n",
       "  48: 'Transportation Equipment',\n",
       "  49: 'Wholesale - Consumer Staples',\n",
       "  50: 'Wholesale - Discretionary'},\n",
       " 'event_type': {0: 'Bankruptcy Filing',\n",
       "  1: 'Default Corp Action',\n",
       "  2: 'Delisting'},\n",
       " 'event_type_subcategory': {0: ' Filing Type: Administration',\n",
       "  1: ' Filing Type: Canadian CCAA',\n",
       "  2: ' Filing Type: Chapter 11',\n",
       "  3: ' Filing Type: Chapter 15',\n",
       "  4: ' Filing Type: Chapter 7',\n",
       "  5: ' Filing Type: Insolvency',\n",
       "  6: ' Filing Type: Japanese CRL',\n",
       "  7: ' Filing Type: Liquidation',\n",
       "  8: ' Filing Type: Pre-Negotiated Chapter 11',\n",
       "  9: ' Filing Type: Protection',\n",
       "  10: ' Filing Type: Receivership',\n",
       "  11: ' Filing Type: Rehabilitation',\n",
       "  12: ' Filing Type: Reorganization',\n",
       "  13: ' Filing Type: Restructuring',\n",
       "  14: ' Filing Type: Unknown',\n",
       "  15: ' Filing Type: Winding Up',\n",
       "  16: ' Reason for delisting: Bankruptcy',\n",
       "  17: ' Reason: ADR (Japan only)',\n",
       "  18: ' Reason: Bankruptcy',\n",
       "  19: ' Reason: Coupon & principal payment',\n",
       "  20: ' Reason: Coupon payment only',\n",
       "  21: ' Reason: Debt Restructuring',\n",
       "  22: ' Reason: Interest payment',\n",
       "  23: ' Reason: Loan payment',\n",
       "  24: ' Reason: Principal payment',\n",
       "  25: ' Reason: Unknown',\n",
       "  26: 'Reason: Buyback option',\n",
       "  27: 'Reason:From CBil (monthly update)',\n",
       "  28: 'Reason:From CBil (new/revised)'},\n",
       " 'event_type_subcategory_sum': {0: 'Bankruptcy',\n",
       "  1: 'Debt Restructuring',\n",
       "  2: 'Insolvency',\n",
       "  3: 'Liquidation',\n",
       "  4: 'Missing Coupon & principal payment',\n",
       "  5: 'Missing Coupon payment only',\n",
       "  6: 'Missing Interest payment',\n",
       "  7: 'Missing Loan payment',\n",
       "  8: 'Missing Principal payment',\n",
       "  9: 'Others',\n",
       "  10: 'Pre-Negotiated Chapter 11',\n",
       "  11: 'Protection',\n",
       "  12: 'Receivership',\n",
       "  13: 'Rehabilitation',\n",
       "  14: 'Restructuring'},\n",
       " 'exit_type': {0: 'Bankruptcy Filing',\n",
       "  1: 'Change in Listing (Exchange to OTC)',\n",
       "  2: 'Default Corp Action',\n",
       "  3: 'Delisting'},\n",
       " 'exit_type_subcategory': {0: ' Exit event: Suspended',\n",
       "  1: ' Filing Type: Canadian CCAA',\n",
       "  2: ' Filing Type: Chapter 11',\n",
       "  3: ' Filing Type: Chapter 15',\n",
       "  4: ' Filing Type: Chapter 7',\n",
       "  5: ' Filing Type: Insolvency',\n",
       "  6: ' Filing Type: Japanese CRL',\n",
       "  7: ' Filing Type: Liquidation',\n",
       "  8: ' Filing Type: Pre-Negotiated Chapter 11',\n",
       "  9: ' Filing Type: Protection',\n",
       "  10: ' Filing Type: Receivership',\n",
       "  11: ' Filing Type: Rehabilitation',\n",
       "  12: ' Filing Type: Restructuring',\n",
       "  13: ' Filing Type: Unknown',\n",
       "  14: ' Filing Type: Winding Up',\n",
       "  15: ' Reason for delisting: Bankruptcy',\n",
       "  16: ' Reason for delisting: Bid Price Below Minimum',\n",
       "  17: ' Reason for delisting: Failure to Meet Listing Requirements',\n",
       "  18: ' Reason for delisting: Insufficient Assets',\n",
       "  19: ' Reason for delisting: Not Available',\n",
       "  20: ' Reason: ADR (Japan only)',\n",
       "  21: ' Reason: Bankruptcy',\n",
       "  22: ' Reason: Coupon & principal payment',\n",
       "  23: ' Reason: Coupon payment only',\n",
       "  24: ' Reason: Debt Restructuring',\n",
       "  25: ' Reason: Interest payment',\n",
       "  26: ' Reason: Loan payment',\n",
       "  27: ' Reason: Principal payment',\n",
       "  28: ' Reason: Unknown',\n",
       "  29: '0',\n",
       "  30: 'Reason: Buyback option',\n",
       "  31: 'Reason:From CBil (monthly update)',\n",
       "  32: 'Reason:From CBil (new/revised)'},\n",
       " 'defaulted_in_last_5_years': {0: 'False', 1: 'True'},\n",
       " 'defaulted_in_last_6_months': {0: 'False', 1: 'True'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency</th>\n",
       "      <th>seniorioty</th>\n",
       "      <th>seniorioty_adj</th>\n",
       "      <th>coupon rate</th>\n",
       "      <th>coupon frequency</th>\n",
       "      <th>maturity_type</th>\n",
       "      <th>call type</th>\n",
       "      <th>put type</th>\n",
       "      <th>Convertible</th>\n",
       "      <th>coupon type -code</th>\n",
       "      <th>...</th>\n",
       "      <th>PD_56_pd</th>\n",
       "      <th>PD_57_pd</th>\n",
       "      <th>PD_58_pd</th>\n",
       "      <th>PD_59_pd</th>\n",
       "      <th>PD_60_pd</th>\n",
       "      <th>DTD</th>\n",
       "      <th>NI_Over_TA</th>\n",
       "      <th>Size</th>\n",
       "      <th>defaulted_in_last_6_months</th>\n",
       "      <th>default_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397453</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.398819</td>\n",
       "      <td>0.399467</td>\n",
       "      <td>0.400092</td>\n",
       "      <td>-0.732815</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.852484</td>\n",
       "      <td>0</td>\n",
       "      <td>6.073973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5.750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.957480</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.957503</td>\n",
       "      <td>0.957514</td>\n",
       "      <td>-1.666262</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-1.186347</td>\n",
       "      <td>0</td>\n",
       "      <td>5.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5.675</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>0</td>\n",
       "      <td>3.145205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>0</td>\n",
       "      <td>3.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130688</td>\n",
       "      <td>0.131081</td>\n",
       "      <td>0.131465</td>\n",
       "      <td>0.131840</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>-0.768857</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-1.946507</td>\n",
       "      <td>0</td>\n",
       "      <td>7.153425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "      <td>6.298630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.950</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "      <td>6.298630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "      <td>6.298630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "      <td>6.298630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.850</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "      <td>6.298630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725 rows × 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      currency  seniorioty  seniorioty_adj  coupon rate  coupon frequency  \\\n",
       "0           15           7               3        9.000               2.0   \n",
       "1           15           7               3        5.750               2.0   \n",
       "2           15          10               6        5.675               2.0   \n",
       "3            1          10               6        0.125               1.0   \n",
       "4            8          10               6        1.750               1.0   \n",
       "...        ...         ...             ...          ...               ...   \n",
       "1720         9           2               2        5.000               2.0   \n",
       "1721         9           2               2        4.950               2.0   \n",
       "1722         9           2               2        5.150               2.0   \n",
       "1723         9           2               2        5.050               2.0   \n",
       "1724         9           2               2        4.850               2.0   \n",
       "\n",
       "      maturity_type  call type  put type  Convertible  coupon type -code  ...  \\\n",
       "0                 3          3         2            1                  7  ...   \n",
       "1                 6          3         2            0                  7  ...   \n",
       "2                11          3         2            1                  3  ...   \n",
       "3                10          3         2            0                  7  ...   \n",
       "4                 6          3         2            1                  7  ...   \n",
       "...             ...        ...       ...          ...                ...  ...   \n",
       "1720              0          3         2            1                  7  ...   \n",
       "1721              0          3         2            1                  7  ...   \n",
       "1722              0          3         2            1                  7  ...   \n",
       "1723              0          3         2            1                  7  ...   \n",
       "1724              0          3         2            1                  7  ...   \n",
       "\n",
       "      PD_56_pd  PD_57_pd  PD_58_pd  PD_59_pd  PD_60_pd       DTD  NI_Over_TA  \\\n",
       "0     0.397453  0.398148  0.398819  0.399467  0.400092 -0.732815   -0.007137   \n",
       "1     0.957467  0.957480  0.957492  0.957503  0.957514 -1.666262   -0.000286   \n",
       "2     0.568693  0.569197  0.569682  0.570150  0.570600 -1.853366    0.000191   \n",
       "3     0.568693  0.569197  0.569682  0.570150  0.570600 -1.853366    0.000191   \n",
       "4     0.130688  0.131081  0.131465  0.131840  0.132206 -0.768857   -0.028058   \n",
       "...        ...       ...       ...       ...       ...       ...         ...   \n",
       "1720  0.081844  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425   \n",
       "1721  0.081844  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425   \n",
       "1722  0.081844  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425   \n",
       "1723  0.081844  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425   \n",
       "1724  0.081844  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425   \n",
       "\n",
       "          Size  defaulted_in_last_6_months  default_duration  \n",
       "0    -0.852484                           0          6.073973  \n",
       "1    -1.186347                           0          5.109589  \n",
       "2     1.053677                           0          3.145205  \n",
       "3     1.053677                           0          3.002740  \n",
       "4    -1.946507                           0          7.153425  \n",
       "...        ...                         ...               ...  \n",
       "1720  2.497169                           0          6.298630  \n",
       "1721  2.497169                           0          6.298630  \n",
       "1722  2.497169                           0          6.298630  \n",
       "1723  2.497169                           0          6.298630  \n",
       "1724  2.497169                           0          6.298630  \n",
       "\n",
       "[1725 rows x 175 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1293, 175)\n",
      "Training Labels Shape: (1293,)\n",
      "Testing Features Shape: (432, 175)\n",
      "Testing Labels Shape: (432,)\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test set\n",
    "test_size = 0.25\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# Prepare the ColumnTransformer\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), non_category_features)   # StandardScaler()\n",
    "    ],\n",
    "    remainder='passthrough'  # Leave categorical features untouched\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUNING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as nn_init\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "import math\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    category_offsets: Optional[Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_numerical: int,\n",
    "        categories: Optional[List[int]],\n",
    "        d_token: int,\n",
    "        bias: bool,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Handling categorical features\n",
    "        if categories is None:\n",
    "            d_bias = d_numerical\n",
    "            self.category_offsets = None\n",
    "            self.category_embeddings = None\n",
    "        else:\n",
    "            d_bias = d_numerical + len(categories)\n",
    "            \n",
    "            # Ensure proper indexing for embeddings\n",
    "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
    "            self.register_buffer('category_offsets', category_offsets)\n",
    "            \n",
    "            # Create category embeddings\n",
    "            self.category_embeddings = nn.Embedding(sum(categories), d_token)\n",
    "        \n",
    "        # Adding [CLS] token into account\n",
    "        self.weight = nn.Parameter(torch.empty(d_numerical + 1, d_token))\n",
    "        self.bias = nn.Parameter(torch.empty(d_bias, d_token)) if bias else None\n",
    "        \n",
    "        # Initialization inspired by nn.Linear\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self) -> None:\n",
    "        \"\"\"Initialize the weights and bias parameters.\"\"\"\n",
    "        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self) -> int:\n",
    "        return len(self.weight) + (0 if self.category_offsets is None else len(self.category_offsets))\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            x_num (Tensor): Numerical input tensor of shape (batch_size, num_features).\n",
    "            x_cat (Optional[Tensor]): Categorical input tensor of shape (batch_size, num_categories).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tokenized output of shape (batch_size, n_tokens, d_token).\n",
    "        \"\"\"\n",
    "        batch_size = x_num.size(0)\n",
    "        \n",
    "        # Add [CLS] token to numerical features\n",
    "        x_num = torch.cat([\n",
    "            torch.ones(batch_size, 1, device=x_num.device),  # [CLS] token\n",
    "            x_num\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Weight numerical features\n",
    "        x = self.weight.unsqueeze(0) * x_num.unsqueeze(-1)   # (1, d_num + 1, d_token) x (batch_size, d_num + 1, 1) => (batch_size, d_num + 1, d_token)\n",
    "        \n",
    "        # Handle categorical features if present\n",
    "        if x_cat is not None:\n",
    "            x_cat_embeddings = self.category_embeddings(x_cat + self.category_offsets.unsqueeze(0)) # (batch_size, num_categories, d_token)\n",
    "            x = torch.cat([x, x_cat_embeddings], dim=1) # batch_size, 1 + d_num + num_Categories, d_token\n",
    "        \n",
    "        # Add bias if applicable\n",
    "        if self.bias is not None:\n",
    "            bias = torch.cat([\n",
    "                torch.zeros(1, self.bias.size(1), device=x.device),\n",
    "                self.bias\n",
    "            ])\n",
    "            x = x + bias.unsqueeze(0)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiheadAttention(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, d: int, n_heads: int, dropout: float, initialization: str\n",
    "#     ) -> None:\n",
    "#         if n_heads > 1:\n",
    "#             assert d % n_heads == 0\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.W_q = nn.Linear(d, d)\n",
    "#         self.W_k = nn.Linear(d, d)\n",
    "#         self.W_v = nn.Linear(d, d)\n",
    "#         self.W_out = nn.Linear(d, d) if n_heads > 1 else None\n",
    "#         self.n_heads = n_heads\n",
    "#         self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "\n",
    "#     def _reshape(self, x: Tensor) -> Tensor:\n",
    "#         batch_size, n_tokens, d = x.shape\n",
    "#         d_head = d // self.n_heads\n",
    "#         return (\n",
    "#             x.reshape(batch_size, n_tokens, self.n_heads, d_head)\n",
    "#             .transpose(1, 2)\n",
    "#             .reshape(batch_size * self.n_heads, n_tokens, d_head)\n",
    "#         )\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x_q: Tensor, #(batch_size, num_tokens, d)\n",
    "#         x_kv: Tensor\n",
    "#     ) -> Tensor:\n",
    "#         # linear projection\n",
    "#         q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n",
    "#         for tensor in [q, k, v]:\n",
    "#             assert tensor.shape[-1] % self.n_heads == 0\n",
    "\n",
    "#         batch_size = len(q)\n",
    "#         d_head_key = k.shape[-1] // self.n_heads\n",
    "#         d_head_value = v.shape[-1] // self.n_heads\n",
    "#         n_q_tokens = q.shape[1]\n",
    "\n",
    "#         q = self._reshape(q)\n",
    "#         k = self._reshape(k)\n",
    "#         attention = F.softmax(q @ k.transpose(1, 2) / math.sqrt(d_head_key), dim=-1)\n",
    "#         if self.dropout is not None:\n",
    "#             attention = self.dropout(attention)\n",
    "#         x = attention @ self._reshape(v)\n",
    "#         x = (\n",
    "#             x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n",
    "#             .transpose(1, 2)\n",
    "#             .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n",
    "#         )\n",
    "#         if self.W_out is not None:\n",
    "#             x = self.W_out(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d: int, n_heads: int, dropout: float = 0.0, initialization: str = \"default\", kv_compression: Optional[float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Multi-head attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            d (int): Dimensionality of the input features.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout probability. Defaults to 0.0.\n",
    "            initialization (str): Initialization method for the linear layers. Defaults to \"default\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure d is divisible by n_heads\n",
    "        assert n_heads > 0 and d % n_heads == 0, \"d must be divisible by n_heads.\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d // n_heads\n",
    "\n",
    "        # Define linear transformations\n",
    "        self.W_q = nn.Linear(d, d)\n",
    "        self.W_k = nn.Linear(d, d)\n",
    "        self.W_v = nn.Linear(d, d)\n",
    "        self.W_out = nn.Linear(d, d) if n_heads > 1 else None\n",
    "        \n",
    "        if kv_compression:\n",
    "            self.key_compression = nn.Linear(d, int(d * kv_compression), bias=False)\n",
    "            self.value_compression = nn.Linear(d, int(d * kv_compression), bias=False)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "        # Initialize weights based on the initialization method\n",
    "        self._initialize_weights(initialization)\n",
    "\n",
    "    def _initialize_weights(self, initialization: str) -> None:\n",
    "        \"\"\"Initialize weights based on the specified method.\"\"\"\n",
    "        layers = [self.W_q, self.W_k, self.W_v, self.W_out]\n",
    "        if self.kv_compression:\n",
    "            layers += [self.key_compression, self.value_compression]\n",
    "            \n",
    "        if initialization == \"xavier\":\n",
    "            for layer in layers:\n",
    "                if layer is not None:\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        nn.init.zeros_(layer.bias)\n",
    "        elif initialization == \"kaiming\":\n",
    "            for layer in [self.W_q, self.W_k, self.W_v, self.W_out]:\n",
    "                if layer is not None:\n",
    "                    nn.init.kaiming_uniform_(layer.weight, a=math.sqrt(5))\n",
    "                    if layer.bias is not None:\n",
    "                        nn.init.zeros_(layer.bias)\n",
    "        else:  # Default initialization\n",
    "            for layer in [self.W_q, self.W_k, self.W_v, self.W_out]:\n",
    "                if layer is not None:\n",
    "                    nn.init.kaiming_uniform_(layer.weight, a=math.sqrt(5))\n",
    "\n",
    "    def _reshape(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Reshape tensor for multi-head attention.\n",
    "\n",
    "        Input:\n",
    "            x: Tensor of shape (batch_size, n_tokens, d)\n",
    "\n",
    "        Output:\n",
    "            Tensor of shape (batch_size * n_heads, n_tokens, d_head)\n",
    "        \"\"\"\n",
    "        batch_size, n_tokens, d = x.shape\n",
    "        return (\n",
    "            x.reshape(batch_size, n_tokens, self.n_heads, self.d_head) \n",
    "            .transpose(1, 2)  # (batch_size, n_heads, n_tokens, d_head)\n",
    "            .reshape(batch_size * self.n_heads, n_tokens, self.d_head)  # (batch_size * n_heads, n_tokens, d_head)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_q: Tensor, x_kv: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            x_q (Tensor): Query tensor of shape (batch_size, num_tokens, d).\n",
    "            x_kv (Tensor): Key and value tensor of shape (batch_size, num_tokens, d).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, num_tokens, d).\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n",
    "        # q: (batch_size, n_q_tokens, d). k, v: (batch_size, num_tokens, d)\n",
    "        \n",
    "        # reduce dimension of key and value if it's too long\n",
    "        if self.kv_compression:\n",
    "            k = self.key_compression(k)\n",
    "            v = self.value_compression(v)\n",
    "\n",
    "        # Ensure dimensions are divisible by number of heads\n",
    "        for tensor in [q, k, v]:\n",
    "            assert tensor.shape[-1] % self.n_heads == 0, \"Input dimensions must be divisible by n_heads.\"\n",
    "\n",
    "        batch_size = q.size(0)\n",
    "        n_q_tokens = q.size(1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = self._reshape(q)  # (batch_size * n_heads, n_q_tokens, d_head)\n",
    "        k = self._reshape(k)  # (batch_size * n_heads, num_tokens, d_head)\n",
    "        v = self._reshape(v)  # (batch_size * n_heads, num_tokens, d_head)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_head)  # (batch_size * n_heads, n_q_tokens, num_tokens)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size * n_heads, n_q_tokens, num_tokens)\n",
    "\n",
    "        # Apply dropout to attention weights if specified\n",
    "        if self.dropout is not None:\n",
    "            attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # Compute attention output\n",
    "        attention_output = attention_weights @ v  # (batch_size * n_heads, n_q_tokens, d_head)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        attention_output = (\n",
    "            attention_output\n",
    "            .reshape(batch_size, self.n_heads, n_q_tokens, self.d_head)  # (batch_size, n_heads, n_q_tokens, d_head)\n",
    "            .transpose(1, 2)  # (batch_size, n_q_tokens, n_heads, d_head)\n",
    "            .reshape(batch_size, n_q_tokens, -1)  # (batch_size, n_q_tokens, d)\n",
    "        )\n",
    "\n",
    "        # Apply final linear transformation if applicable\n",
    "        if self.W_out is not None:\n",
    "            attention_output = self.W_out(attention_output)  # (batch_size, n_q_tokens, d)\n",
    "\n",
    "        return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reglu(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"ReGLU activation function.\"\"\"\n",
    "    a, b = x.chunk(2, dim=-1)\n",
    "    return a * F.relu(b)\n",
    "\n",
    "def geglu(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"GeGLU activation function.\"\"\"\n",
    "    a, b = x.chunk(2, dim=-1)\n",
    "    return a * F.gelu(b)\n",
    "\n",
    "def get_activation_fn(name: str) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Retrieve the activation function by name.\"\"\"\n",
    "    activation_functions = {\n",
    "        'reglu': reglu,\n",
    "        'geglu': geglu,\n",
    "        'sigmoid': torch.sigmoid,\n",
    "    }\n",
    "\n",
    "    if name in activation_functions:\n",
    "        return activation_functions[name]\n",
    "\n",
    "    if hasattr(F, name):\n",
    "        return getattr(F, name)\n",
    "\n",
    "    raise ValueError(f\"Unsupported activation function: {name}\")\n",
    "\n",
    "def get_nonglu_activation_fn(name: str) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Retrieve the non-GLU activation function by name.\"\"\"\n",
    "    nonglu_mappings = {\n",
    "        'reglu': F.relu,\n",
    "        'geglu': F.gelu,\n",
    "    }\n",
    "\n",
    "    return nonglu_mappings.get(name, get_activation_fn(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Transformer(nn.Module):\n",
    "#     \"\"\"Transformer.\n",
    "\n",
    "#     References:\n",
    "#     - https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "#     - https://github.com/facebookresearch/pytext/tree/master/pytext/models/representations/transformer\n",
    "#     - https://github.com/pytorch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/linformer/linformer_src/modules/multihead_linear_attention.py#L19\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         *,\n",
    "#         # tokenizer\n",
    "#         d_numerical: int,\n",
    "#         categories: Optional[List[int]],\n",
    "#         token_bias: bool,\n",
    "#         # transformer\n",
    "#         n_layers: int,\n",
    "#         d_token: int,\n",
    "#         n_heads: int,\n",
    "#         d_ffn_factor: float,\n",
    "#         attention_dropout: float,\n",
    "#         ffn_dropout: float,\n",
    "#         residual_dropout: float,\n",
    "#         activation: str,\n",
    "#         prenormalization: bool,\n",
    "#         initialization: str,\n",
    "#         # linformer\n",
    "#         kv_compression: Optional[float],\n",
    "#         kv_compression_sharing: Optional[str],\n",
    "#         #\n",
    "#         d_out: int,\n",
    "#     ) -> None:\n",
    "#         assert (kv_compression is None) ^ (kv_compression_sharing is not None)\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = Tokenizer(d_numerical, categories, d_token, token_bias)\n",
    "#         n_tokens = self.tokenizer.n_tokens\n",
    "\n",
    "#         def make_kv_compression():\n",
    "#             assert kv_compression\n",
    "#             compression = nn.Linear(\n",
    "#                 n_tokens, int(n_tokens * kv_compression), bias=False\n",
    "#             )\n",
    "#             if initialization == 'xavier':\n",
    "#                 nn_init.xavier_uniform_(compression.weight)\n",
    "#             return compression\n",
    "\n",
    "#         self.shared_kv_compression = (\n",
    "#             make_kv_compression()\n",
    "#             if kv_compression and kv_compression_sharing == 'layerwise'\n",
    "#             else None\n",
    "#         )\n",
    "\n",
    "#         def make_normalization():\n",
    "#             return nn.LayerNorm(d_token)\n",
    "\n",
    "#         d_hidden = int(d_token * d_ffn_factor)\n",
    "#         self.layers = nn.ModuleList([])\n",
    "#         for layer_idx in range(n_layers):\n",
    "#             layer = nn.ModuleDict(\n",
    "#                 {\n",
    "#                     'attention': MultiheadAttention(\n",
    "#                         d_token, n_heads, attention_dropout, initialization\n",
    "#                     ),\n",
    "#                     'linear0': nn.Linear(\n",
    "#                         d_token, d_hidden * (2 if activation.endswith('glu') else 1)\n",
    "#                     ),\n",
    "#                     'linear1': nn.Linear(d_hidden, d_token),\n",
    "#                     'norm1': make_normalization(),\n",
    "#                 }\n",
    "#             )\n",
    "#             if not prenormalization or layer_idx:\n",
    "#                 layer['norm0'] = make_normalization()\n",
    "#             if kv_compression and self.shared_kv_compression is None:\n",
    "#                 layer['key_compression'] = make_kv_compression()\n",
    "#                 if kv_compression_sharing == 'headwise':\n",
    "#                     layer['value_compression'] = make_kv_compression()\n",
    "#                 else:\n",
    "#                     assert kv_compression_sharing == 'key-value'\n",
    "#             self.layers.append(layer)\n",
    "\n",
    "#         self.activation = get_activation_fn(activation)\n",
    "#         self.last_activation = get_nonglu_activation_fn(activation)\n",
    "#         self.prenormalization = prenormalization\n",
    "#         self.last_normalization = make_normalization() if prenormalization else None\n",
    "#         self.ffn_dropout = ffn_dropout\n",
    "#         self.residual_dropout = residual_dropout\n",
    "#         self.head = nn.Linear(d_token, d_out)\n",
    "\n",
    "#     def _get_kv_compressions(self, layer):\n",
    "#         return (\n",
    "#             (self.shared_kv_compression, self.shared_kv_compression)\n",
    "#             if self.shared_kv_compression is not None\n",
    "#             else (layer['key_compression'], layer['value_compression'])\n",
    "#             if 'key_compression' in layer and 'value_compression' in layer\n",
    "#             else (layer['key_compression'], layer['key_compression'])\n",
    "#             if 'key_compression' in layer\n",
    "#             else (None, None)\n",
    "#         )\n",
    "\n",
    "#     def _start_residual(self, x, layer, norm_idx):\n",
    "#         x_residual = x\n",
    "#         if self.prenormalization:\n",
    "#             norm_key = f'norm{norm_idx}'\n",
    "#             if norm_key in layer:\n",
    "#                 x_residual = layer[norm_key](x_residual)\n",
    "#         return x_residual\n",
    "\n",
    "#     def _end_residual(self, x, x_residual, layer, norm_idx):\n",
    "#         if self.residual_dropout:\n",
    "#             x_residual = F.dropout(x_residual, self.residual_dropout, self.training)\n",
    "#         x = x + x_residual\n",
    "#         if not self.prenormalization:\n",
    "#             x = layer[f'norm{norm_idx}'](x)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x_num: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "#         x = self.tokenizer(x_num, x_cat)\n",
    "\n",
    "#         for layer_idx, layer in enumerate(self.layers):\n",
    "#             is_last_layer = layer_idx + 1 == len(self.layers)\n",
    "#             layer = cast(Dict[str, nn.Module], layer)\n",
    "\n",
    "#             # start residual connection\n",
    "#             x_residual = self._start_residual(x, layer, 0)\n",
    "            \n",
    "#             # attention layer\n",
    "#             x_residual = layer['attention'](\n",
    "#                 # for the last attention, it is enough to process only [CLS]\n",
    "#                 (x_residual[:, :1] if is_last_layer else x_residual),\n",
    "#                 x_residual,\n",
    "#                 *self._get_kv_compressions(layer),\n",
    "#             )\n",
    "            \n",
    "#             # end residual connection\n",
    "#             if is_last_layer:\n",
    "#                 x = x[:, : x_residual.shape[1]]\n",
    "#             x = self._end_residual(x, x_residual, layer, 0)\n",
    "\n",
    "#             # feedforward network\n",
    "#             x_residual = self._start_residual(x, layer, 1)\n",
    "#             x_residual = layer['linear0'](x_residual)\n",
    "#             x_residual = self.activation(x_residual)\n",
    "#             if self.ffn_dropout:\n",
    "#                 x_residual = F.dropout(x_residual, self.ffn_dropout, self.training)\n",
    "#             x_residual = layer['linear1'](x_residual)\n",
    "#             x = self._end_residual(x, x_residual, layer, 1)\n",
    "\n",
    "#         # final layer processing\n",
    "#         assert x.shape[1] == 1\n",
    "#         x = x[:, 0]\n",
    "#         if self.last_normalization is not None:\n",
    "#             x = self.last_normalization(x)\n",
    "#         x = self.last_activation(x)\n",
    "#         x = self.head(x)\n",
    "#         x = x.squeeze(-1)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer implementation with support for optional Linformer compression.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_numerical: int,\n",
    "        categories: Optional[List[int]],\n",
    "        token_bias: bool,\n",
    "        n_layers: int,\n",
    "        d_token: int,\n",
    "        n_heads: int,\n",
    "        d_ffn_factor: float,\n",
    "        attention_dropout: float,\n",
    "        ffn_dropout: float,\n",
    "        residual_dropout: float,\n",
    "        activation: str,\n",
    "        prenormalization: bool,\n",
    "        initialization: str,\n",
    "        kv_compression: Optional[float],\n",
    "        kv_compression_sharing: Optional[str],\n",
    "        d_out: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Validate kv_compression settings\n",
    "        if (kv_compression is None) != (kv_compression_sharing is None):\n",
    "            raise ValueError(\"Both or neither of kv_compression and kv_compression_sharing must be set.\")\n",
    "\n",
    "        self.tokenizer = Tokenizer(d_numerical, categories, d_token, token_bias)\n",
    "        n_tokens = self.tokenizer.n_tokens\n",
    "\n",
    "        # kv_compression parameter introduces a linear transformation that compresses the key and value matrices to a smaller dimension\n",
    "        # Layerwise: A single shared compression matrix is used across all layers.\n",
    "        self.shared_kv_compression = (\n",
    "            self._make_kv_compression(n_tokens, kv_compression, initialization)\n",
    "            if kv_compression and kv_compression_sharing == 'layerwise'\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        d_hidden = int(d_token * d_ffn_factor)\n",
    "        self.layers = nn.ModuleList([\n",
    "            self._make_layer(\n",
    "                d_token,\n",
    "                d_hidden,\n",
    "                n_heads,\n",
    "                attention_dropout,\n",
    "                activation,\n",
    "                prenormalization,\n",
    "                initialization,\n",
    "                n_tokens,\n",
    "                kv_compression,\n",
    "                kv_compression_sharing\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.activation = get_activation_fn(activation)\n",
    "        \n",
    "        # use non-glu activation function in the last layer for simplicity\n",
    "        self.last_activation = get_nonglu_activation_fn(activation)\n",
    "        self.prenormalization = prenormalization\n",
    "        self.last_normalization = nn.LayerNorm(d_token) if prenormalization else None\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.residual_dropout = residual_dropout\n",
    "        self.head = nn.Linear(d_token, d_out)\n",
    "\n",
    "    def _make_kv_compression(self, n_tokens, kv_compression, initialization):\n",
    "        \"\"\"Create a key-value compression layer.\"\"\"\n",
    "        compression = nn.Linear(n_tokens, int(n_tokens * kv_compression), bias=False)\n",
    "        if initialization == 'xavier':\n",
    "            nn.init.xavier_uniform_(compression.weight)\n",
    "        return compression\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        d_token,\n",
    "        d_hidden,\n",
    "        n_heads,\n",
    "        attention_dropout,\n",
    "        activation,\n",
    "        prenormalization,\n",
    "        initialization,\n",
    "        n_tokens,\n",
    "        kv_compression,\n",
    "        kv_compression_sharing\n",
    "    ) -> nn.ModuleDict:\n",
    "        \"\"\"Construct a Transformer layer.\"\"\"\n",
    "        layer = nn.ModuleDict({\n",
    "            'attention': MultiheadAttention(\n",
    "                d_token, n_heads, attention_dropout, initialization, kv_compression\n",
    "            ),\n",
    "            'linear0': nn.Linear(\n",
    "                d_token, d_hidden * (2 if activation.endswith('glu') else 1)\n",
    "            ),\n",
    "            'linear1': nn.Linear(d_hidden, d_token),\n",
    "            'norm1': nn.LayerNorm(d_token),\n",
    "        })\n",
    "\n",
    "        if not prenormalization:\n",
    "            layer['norm0'] = nn.LayerNorm(d_token)\n",
    "\n",
    "        if kv_compression and kv_compression_sharing != 'layerwise':\n",
    "            layer['key_compression'] = self._make_kv_compression(n_tokens, kv_compression, initialization)\n",
    "            if kv_compression_sharing == 'headwise':\n",
    "                layer['value_compression'] = self._make_kv_compression(n_tokens, kv_compression, initialization)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x_num: torch.Tensor, x_cat: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Transformer model.\"\"\"\n",
    "        x = self.tokenizer(x_num, x_cat)\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            is_last_layer = layer_idx + 1 == len(self.layers)\n",
    "\n",
    "            # Start residual connection\n",
    "            x_residual = self._start_residual(x, layer, 0)\n",
    "\n",
    "            # Attention\n",
    "            x_residual = layer['attention'](\n",
    "                x_residual[:, :1] if is_last_layer else x_residual,\n",
    "                x_residual\n",
    "            )\n",
    "\n",
    "            # End residual connection\n",
    "            x = self._end_residual(x, x_residual, layer, 0)\n",
    "\n",
    "            # Feedforward\n",
    "            x_residual = self._start_residual(x, layer, 1)\n",
    "            x_residual = self.activation(layer['linear0'](x_residual))\n",
    "\n",
    "            if self.ffn_dropout:\n",
    "                x_residual = F.dropout(x_residual, self.ffn_dropout, self.training)\n",
    "\n",
    "            x_residual = layer['linear1'](x_residual)\n",
    "            x = self._end_residual(x, x_residual, layer, 1)\n",
    "\n",
    "        # Final normalization and activation\n",
    "        x = x[:, 0]\n",
    "        if self.last_normalization is not None:\n",
    "            x = self.last_normalization(x)\n",
    "        x = self.last_activation(x)\n",
    "\n",
    "        return self.head(x)\n",
    "\n",
    "    def _get_kv_compressions(self, layer):\n",
    "        \"\"\"Retrieve key-value compression modules for a layer.\"\"\"\n",
    "        return (\n",
    "            (self.shared_kv_compression, self.shared_kv_compression)\n",
    "            if self.shared_kv_compression\n",
    "            else (\n",
    "                layer.get('key_compression', None),\n",
    "                layer.get('value_compression', layer.get('key_compression', None))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _start_residual(self, x, layer, norm_idx):\n",
    "        \"\"\"Start a residual connection.\"\"\"\n",
    "        if self.prenormalization:\n",
    "            norm_key = f'norm{norm_idx}'\n",
    "            return layer[norm_key](x) if norm_key in layer else x\n",
    "        return x\n",
    "\n",
    "    def _end_residual(self, x, x_residual, layer, norm_idx):\n",
    "        \"\"\"End a residual connection.\"\"\"\n",
    "        if self.residual_dropout:\n",
    "            x_residual = F.dropout(x_residual, self.residual_dropout, self.training)\n",
    "        x = x + x_residual\n",
    "        if not self.prenormalization:\n",
    "            x = layer[f'norm{norm_idx}'](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 7, 27, 13, 11, 19, 51, 3, 15, 2, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(le.classes_) for le in label_encoders.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    # model = define_model(trial, train_features.shape[1], 1).to(DEVICE)\n",
    "    # Define out_features_list\n",
    "    n_heads = trial.suggest_int(\"n_heads\", 1, 10)\n",
    "    \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    \n",
    "    token_multiplier = trial.suggest_int(\"token_multiplier\", 5, 30)  # Adjust the range as necessary\n",
    "    \n",
    "    # Suggest an integer for a that is divisible by b\n",
    "    d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n",
    "    \n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0, 0.5)\n",
    "    d_ffn_factor = trial.suggest_float(\"d_ffn_factor\", 1, 3)\n",
    "    ffn_dropout = trial.suggest_float(\"ffn_dropout\", 0, 0.5)\n",
    "    \n",
    "    # activation = trial.suggest_categorical(\"activation\", choices=['relu', 'reglu', 'geglu'])\n",
    "    # batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
    "    \n",
    "    categories = [len(le.classes_) for le in label_encoders.values()]\n",
    "    d_numerical = len(non_category_features)\n",
    "\n",
    "    args = {'activation': 'relu', #activation, #\n",
    "    'attention_dropout': attention_dropout,\n",
    "    'd_ffn_factor': d_ffn_factor,\n",
    "    'd_token': d_token,\n",
    "    'ffn_dropout': ffn_dropout,\n",
    "    'initialization': 'kaiming',\n",
    "    'n_heads': n_heads,\n",
    "    'n_layers': n_layers,\n",
    "    'prenormalization': False,\n",
    "    'residual_dropout': 0.0,\n",
    "    'kv_compression': None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    'token_bias': True,\n",
    "    'd_out': 1\n",
    "    }\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-9, 1e-1, log=True)\n",
    "\n",
    "    # training with 5-fold CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_features):\n",
    "        # Create training and validation datasets for the current fold\n",
    "        X_train_fold, X_val_fold = train_features.iloc[train_idx], train_features.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = train_labels.iloc[train_idx], train_labels.iloc[val_idx]\n",
    "        \n",
    "        # scaling features\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "            \n",
    "        # Initialize the model for this fold\n",
    "        model = Transformer(d_numerical=d_numerical, categories=categories, **args).to(DEVICE)\n",
    "        model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # define optimizer\n",
    "        if optimizer_name == \"Adam\":\n",
    "         optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            momentum = trial.suggest_float(\"momentum\", 1e-9, 0.95, log=True)\n",
    "            optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        \n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Prepare DataLoader for training\n",
    "        train_dataset = CustomDataset(X_train_fold, y_train_fold.to_numpy())\n",
    "        val_dataset = CustomDataset(X_val_fold, y_val_fold.to_numpy())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            \n",
    "        # Training of the model.\n",
    "        model.train()\n",
    "        for epoch in range(EPOCHS):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                \n",
    "                X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "                X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_num, X_cat)\n",
    "\n",
    "                # print(\"shape\", output.shape, target.shape)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "                X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "                output = model(X_num, X_cat)\n",
    "                val_loss = criterion(output, target).item()\n",
    "                val_losses.append(val_loss**0.5) #rmse\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "    # Return the average validation loss across all folds\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-27 03:59:38,680] A new study created in memory with name: no-name-e5f08483-18f9-496d-961d-f3f73e36a323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63241/1311913245.py:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-08-27 04:03:49,651] Trial 0 finished with value: 0.2778907221754452 and parameters: {'n_heads': 5, 'n_layers': 3, 'token_multiplier': 13, 'd_token': 5, 'attention_dropout': 0.47357853391284443, 'd_ffn_factor': 1.5567463702685527, 'ffn_dropout': 0.09703116655822941, 'optimizer': 'RMSprop', 'lr': 0.00045523806322604355, 'weight_decay': 5.01534556567381e-07, 'momentum': 8.456152924489299e-07}. Best is trial 0 with value: 0.2778907221754452.\n",
      "/tmp/ipykernel_63241/1311913245.py:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-08-27 04:07:01,453] Trial 1 finished with value: 0.3261027943937555 and parameters: {'n_heads': 10, 'n_layers': 3, 'token_multiplier': 13, 'd_token': 40, 'attention_dropout': 0.11634236810833715, 'd_ffn_factor': 1.4151651199217234, 'ffn_dropout': 0.20928743701569774, 'optimizer': 'Adam', 'lr': 0.009410155719751772, 'weight_decay': 0.09667794268657083}. Best is trial 0 with value: 0.2778907221754452.\n",
      "/tmp/ipykernel_63241/1311913245.py:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-08-27 04:10:04,882] Trial 2 finished with value: 0.2516675406673338 and parameters: {'n_heads': 5, 'n_layers': 2, 'token_multiplier': 20, 'd_token': 60, 'attention_dropout': 0.09192629672301816, 'd_ffn_factor': 2.565011542335736, 'ffn_dropout': 0.07564072635765423, 'optimizer': 'RMSprop', 'lr': 0.00022923558072776028, 'weight_decay': 7.433796546124456e-05, 'momentum': 2.161429700687457e-05}. Best is trial 2 with value: 0.2516675406673338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  3\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  3\n",
      "Best trial:\n",
      "  Value:  0.2516675406673338\n",
      "  Params: \n",
      "    n_heads: 5\n",
      "    n_layers: 2\n",
      "    token_multiplier: 20\n",
      "    d_token: 60\n",
      "    attention_dropout: 0.09192629672301816\n",
      "    d_ffn_factor: 2.565011542335736\n",
      "    ffn_dropout: 0.07564072635765423\n",
      "    optimizer: RMSprop\n",
      "    lr: 0.00022923558072776028\n",
      "    weight_decay: 7.433796546124456e-05\n",
      "    momentum: 2.161429700687457e-05\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_heads': 5,\n",
       " 'n_layers': 2,\n",
       " 'token_multiplier': 20,\n",
       " 'd_token': 60,\n",
       " 'attention_dropout': 0.09192629672301816,\n",
       " 'd_ffn_factor': 2.565011542335736,\n",
       " 'ffn_dropout': 0.07564072635765423,\n",
       " 'optimizer': 'RMSprop',\n",
       " 'lr': 0.00022923558072776028,\n",
       " 'weight_decay': 7.433796546124456e-05,\n",
       " 'momentum': 2.161429700687457e-05}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\"model\": {}, \"optimizer\": {}}\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    if key in ['lr', 'weight_decay', 'momentum', 'weight_decay', 'optimizer']:\n",
    "        MODEL_CONFIG[\"optimizer\"][key] = value\n",
    "    elif key == 'token_multiplier':\n",
    "        continue\n",
    "    elif key == 'batch_size':\n",
    "        BATCH_SIZE = value\n",
    "    else:\n",
    "        # adj_key = key.rpartition('_')[0]\n",
    "        MODEL_CONFIG[\"model\"][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'n_heads': 5,\n",
       "  'n_layers': 2,\n",
       "  'd_token': 60,\n",
       "  'attention_dropout': 0.09192629672301816,\n",
       "  'd_ffn_factor': 2.565011542335736,\n",
       "  'ffn_dropout': 0.07564072635765423},\n",
       " 'optimizer': {'optimizer': 'RMSprop',\n",
       "  'lr': 0.00022923558072776028,\n",
       "  'weight_decay': 7.433796546124456e-05,\n",
       "  'momentum': 2.161429700687457e-05}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "new_feature_list = non_category_features + category_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = CustomDataset(train_features, train_labels.to_numpy())\n",
    "test_dataset = CustomDataset(test_features, test_labels.to_numpy())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_features.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    }
   ],
   "source": [
    "categories = [len(le.classes_) for le in label_encoders.values()]\n",
    "d_numerical = len(non_category_features)\n",
    "# d_token = 32  # Example token dimension\n",
    "# token_bias = True\n",
    "\n",
    "args = {\n",
    "  'initialization': 'kaiming',\n",
    "  'activation': 'relu',\n",
    "    'prenormalization': False,\n",
    "    'residual_dropout': 0.0,\n",
    "    'kv_compression': None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    'token_bias': True,\n",
    "    'd_out': 1\n",
    "}\n",
    "\n",
    "args.update(MODEL_CONFIG[\"model\"])\n",
    "model = Transformer(d_numerical=d_numerical, categories=categories, **args).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tokenizer): Tokenizer(\n",
       "    (category_embeddings): Embedding(166, 60)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x ModuleDict(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (W_k): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (W_v): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (W_out): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (dropout): Dropout(p=0.09192629672301816, inplace=False)\n",
       "      )\n",
       "      (linear0): Linear(in_features=60, out_features=153, bias=True)\n",
       "      (linear1): Linear(in_features=153, out_features=60, bias=True)\n",
       "      (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=60, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Transformer(\n",
       "    (tokenizer): Tokenizer(\n",
       "      (category_embeddings): Embedding(166, 60)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x ModuleDict(\n",
       "        (attention): MultiheadAttention(\n",
       "          (W_q): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (W_k): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (W_v): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (W_out): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (dropout): Dropout(p=0.09192629672301816, inplace=False)\n",
       "        )\n",
       "        (linear0): Linear(in_features=60, out_features=153, bias=True)\n",
       "        (linear1): Linear(in_features=153, out_features=60, bias=True)\n",
       "        (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (head): Linear(in_features=60, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE != \"cpu\":\n",
    "    model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.00022923558072776028\n",
       "    maximize: False\n",
       "    momentum: 2.161429700687457e-05\n",
       "    weight_decay: 7.433796546124456e-05\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define optimizer\n",
    "optim_config = deepcopy(MODEL_CONFIG[\"optimizer\"])\n",
    "del optim_config[\"optimizer\"]\n",
    "\n",
    "optimizer = getattr(optim, MODEL_CONFIG[\"optimizer\"][\"optimizer\"])(model.parameters(), **optim_config)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:35<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.1137\n",
      "Training time: 515.221 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 500\n",
    "criterion = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "\n",
    "for ep in tqdm(range(EPOCH)):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "        X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X_num, X_cat)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() * target.size(0)\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{ep + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "\n",
    "\n",
    "    train_loss = running_loss  / len(train_loader.dataset)\n",
    "    \n",
    "train_loss = running_loss  / len(train_loader.dataset)\n",
    "print(f'Epoch [{ep+1}], Train Loss: {train_loss**0.5:.4f}')\n",
    "\n",
    "# print out training time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training time: {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training average mean absolute error: 0.15283623337745667\n",
      "Training average mean absolute percentage error: 312.3091459274292\n",
      "Training average root mean squared error: 0.23530429349573925\n",
      "Training average R2: 0.516621470451355\n"
     ]
    }
   ],
   "source": [
    "# Testing phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for data, target in test_loader:\n",
    "        # inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        X_num = data[:, :len(non_category_features)]\n",
    "        X_cat = data[:, -len(category_features):].detach().long()\n",
    "        \n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model = model.module  # Unwrap from DataParallel\n",
    "        model = model.to('cpu')\n",
    "            \n",
    "               \n",
    "        outputs = model(X_num, X_cat)\n",
    "\n",
    "        # save metrics\n",
    "        mae, mape, rmse, rsqr = calculate_metric(outputs.numpy(), target.numpy())\n",
    "        print(f\"Training average mean absolute error: {mae}\")\n",
    "        print(f\"Training average mean absolute percentage error: {mape}\")\n",
    "        print(f\"Training average root mean squared error: {rmse}\")\n",
    "        print(f\"Training average R2: {rsqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s load back in our saved model\n",
    "# model = MLP()\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 164)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currency                        int64\n",
       "seniorioty_adj                  int64\n",
       "coupon rate                   float64\n",
       "domicile_country                int64\n",
       "exchange_country                int64\n",
       "                               ...   \n",
       "PD_60_pd                      float64\n",
       "DTD                           float64\n",
       "NI_Over_TA                    float64\n",
       "Size                          float64\n",
       "defaulted_in_last_6_months      int64\n",
       "Length: 164, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    }
   ],
   "source": [
    "# Define cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "EPOCHS = 500\n",
    "val_mae = []\n",
    "val_mape = []\n",
    "val_rmse = []\n",
    "val_rsqr = []\n",
    "\n",
    "\n",
    "for train_idx, val_idx in kf.split(features):\n",
    "    # Create training and validation datasets for the current fold\n",
    "    X_train_fold, X_val_fold = features.iloc[train_idx], features.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = labels.iloc[train_idx], labels.iloc[val_idx]\n",
    "    \n",
    "    # scaling features\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "        \n",
    "    # Initialize the model for this fold\n",
    "    model = Transformer(d_numerical=d_numerical, categories=categories, **args)\n",
    "    model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = getattr(optim, MODEL_CONFIG[\"optimizer\"][\"optimizer\"])(model.parameters(), **optim_config)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Prepare DataLoader for training\n",
    "    train_dataset = CustomDataset(X_train_fold, y_train_fold.to_numpy())\n",
    "    val_dataset = CustomDataset(X_val_fold, y_val_fold.to_numpy())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_dataset.features.shape[0], shuffle=True)\n",
    "        \n",
    "    # Training of the model.\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "  \n",
    "            X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "            X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(X_num, X_cat)\n",
    "\n",
    "            \n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(f'Epoch [{ep+1}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Validation of the model.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            X_num = data[:, :len(non_category_features)]\n",
    "            X_cat = data[:, -len(category_features):].detach().long()\n",
    "            \n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                model = model.module  # Unwrap from DataParallel\n",
    "            model = model.to('cpu')\n",
    "            outputs = model(X_num, X_cat)\n",
    "            \n",
    "            # save metrics\n",
    "            mae, mape, rmse, rsqr = calculate_metric(outputs.numpy(), target.numpy())\n",
    "            val_mae.append(mae)\n",
    "            val_mape.append(mape)\n",
    "            val_rmse.append(rmse)\n",
    "            val_rsqr.append(rsqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average mean absolute error: 0.16323910653591156\n",
      "Test average mean absolute percentage error: 5443.055862188339\n",
      "Test average root mean squared error: 0.24957806415090034\n",
      "Test average R2: 0.42442349195480344\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test average mean absolute error: {statistics.mean(val_mae)}\")\n",
    "print(f\"Test average mean absolute percentage error: {statistics.mean(val_mape)}\")\n",
    "print(f\"Test average root mean squared error: {statistics.mean(val_rmse)}\")\n",
    "print(f\"Test average R2: {statistics.mean(val_rsqr)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
