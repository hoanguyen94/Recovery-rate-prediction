{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Optional, Callable, cast, Dict\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# import typing as ty\n",
    "from torch import Tensor\n",
    "import torch.nn.init as nn_init\n",
    "import statistics\n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"../data/data_removing_na.xlsx\"\n",
    "\n",
    "MODEL_PATH = 'best_transformer_model.pth'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = 'cpu'\n",
    "DEVICE_LIST = [0, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rr1_30</th>\n",
       "      <th>currency</th>\n",
       "      <th>seniorioty_adj</th>\n",
       "      <th>coupon rate</th>\n",
       "      <th>domicile_country</th>\n",
       "      <th>exchange_country</th>\n",
       "      <th>Industry_sector</th>\n",
       "      <th>Industry_group</th>\n",
       "      <th>Industry_subgroup</th>\n",
       "      <th>event_type</th>\n",
       "      <th>...</th>\n",
       "      <th>PD_55_pd</th>\n",
       "      <th>PD_56_pd</th>\n",
       "      <th>PD_57_pd</th>\n",
       "      <th>PD_58_pd</th>\n",
       "      <th>PD_59_pd</th>\n",
       "      <th>PD_60_pd</th>\n",
       "      <th>DTD</th>\n",
       "      <th>NI_Over_TA</th>\n",
       "      <th>Size</th>\n",
       "      <th>defaulted_in_last_6_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.259908</td>\n",
       "      <td>USD</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>9.000</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Retail &amp; Whsle - Discretionary</td>\n",
       "      <td>E-Commerce Discretionary</td>\n",
       "      <td>Bankruptcy Filing</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396731</td>\n",
       "      <td>0.397453</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.398819</td>\n",
       "      <td>0.399467</td>\n",
       "      <td>0.400092</td>\n",
       "      <td>-0.732815</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.852484</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.032729</td>\n",
       "      <td>USD</td>\n",
       "      <td>Senior Subordinated Unsecured</td>\n",
       "      <td>5.750</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Facilities &amp; Svcs</td>\n",
       "      <td>Default Corp Action</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957454</td>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.957480</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.957503</td>\n",
       "      <td>0.957514</td>\n",
       "      <td>-1.666262</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-1.186347</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972400</td>\n",
       "      <td>USD</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>5.675</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Retail &amp; Whsle - Discretionary</td>\n",
       "      <td>Wholesale - Discretionary</td>\n",
       "      <td>Default Corp Action</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568169</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.047416</td>\n",
       "      <td>CHF</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>0.125</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Retail &amp; Whsle - Discretionary</td>\n",
       "      <td>Wholesale - Discretionary</td>\n",
       "      <td>Default Corp Action</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568169</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.848872</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>1.750</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Products</td>\n",
       "      <td>Electrical Equipment</td>\n",
       "      <td>Bankruptcy Filing</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130285</td>\n",
       "      <td>0.130688</td>\n",
       "      <td>0.131081</td>\n",
       "      <td>0.131465</td>\n",
       "      <td>0.131840</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>-0.768857</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-1.946507</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rr1_30 currency                 seniorioty_adj  coupon rate  \\\n",
       "0  0.259908      USD  Senior Subordinated Unsecured        9.000   \n",
       "1  0.032729      USD  Senior Subordinated Unsecured        5.750   \n",
       "2  0.972400      USD                      Unsecured        5.675   \n",
       "3  1.047416      CHF                      Unsecured        0.125   \n",
       "4  0.848872      JPY                      Unsecured        1.750   \n",
       "\n",
       "  domicile_country exchange_country         Industry_sector  \\\n",
       "0    United States    United States  Consumer Discretionary   \n",
       "1    United States    United States             Health Care   \n",
       "2      South Korea      South Korea  Consumer Discretionary   \n",
       "3      South Korea      South Korea  Consumer Discretionary   \n",
       "4            Japan            Japan             Industrials   \n",
       "\n",
       "                   Industry_group              Industry_subgroup  \\\n",
       "0  Retail & Whsle - Discretionary       E-Commerce Discretionary   \n",
       "1                     Health Care  Health Care Facilities & Svcs   \n",
       "2  Retail & Whsle - Discretionary      Wholesale - Discretionary   \n",
       "3  Retail & Whsle - Discretionary      Wholesale - Discretionary   \n",
       "4             Industrial Products           Electrical Equipment   \n",
       "\n",
       "            event_type  ...  PD_55_pd  PD_56_pd  PD_57_pd  PD_58_pd  PD_59_pd  \\\n",
       "0    Bankruptcy Filing  ...  0.396731  0.397453  0.398148  0.398819  0.399467   \n",
       "1  Default Corp Action  ...  0.957454  0.957467  0.957480  0.957492  0.957503   \n",
       "2  Default Corp Action  ...  0.568169  0.568693  0.569197  0.569682  0.570150   \n",
       "3  Default Corp Action  ...  0.568169  0.568693  0.569197  0.569682  0.570150   \n",
       "4    Bankruptcy Filing  ...  0.130285  0.130688  0.131081  0.131465  0.131840   \n",
       "\n",
       "   PD_60_pd       DTD  NI_Over_TA      Size  defaulted_in_last_6_months  \n",
       "0  0.400092 -0.732815   -0.007137 -0.852484                       False  \n",
       "1  0.957514 -1.666262   -0.000286 -1.186347                       False  \n",
       "2  0.570600 -1.853366    0.000191  1.053677                       False  \n",
       "3  0.570600 -1.853366    0.000191  1.053677                       False  \n",
       "4  0.132206 -0.768857   -0.028058 -1.946507                       False  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 165)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['rr1_30']\n",
    "features = df.drop(columns='rr1_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currency                       object\n",
       "seniorioty_adj                 object\n",
       "coupon rate                   float64\n",
       "domicile_country               object\n",
       "exchange_country               object\n",
       "                               ...   \n",
       "PD_60_pd                      float64\n",
       "DTD                           float64\n",
       "NI_Over_TA                    float64\n",
       "Size                          float64\n",
       "defaulted_in_last_6_months       bool\n",
       "Length: 164, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = features.columns\n",
    "category_features = list(features.select_dtypes(include=['object', 'bool']).columns)\n",
    "non_category_features = [i for i in feature_list if i not in category_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(non_category_features))\n",
    "print(len(category_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['currency',\n",
       " 'seniorioty_adj',\n",
       " 'domicile_country',\n",
       " 'exchange_country',\n",
       " 'Industry_sector',\n",
       " 'Industry_group',\n",
       " 'Industry_subgroup',\n",
       " 'event_type',\n",
       " 'event_type_subcategory_sum',\n",
       " 'defaulted_in_last_5_years',\n",
       " 'defaulted_in_last_6_months']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical features in features\n",
    "label_encoders = {}\n",
    "mappings = {}\n",
    "\n",
    "for column in category_features:\n",
    "    le = LabelEncoder()\n",
    "    features[column] = le.fit_transform(features[column])\n",
    "    label_encoders[column] = le\n",
    "    mappings[column] = {index: label for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currency': {0: 'CAD',\n",
       "  1: 'CHF',\n",
       "  2: 'CNY',\n",
       "  3: 'EUR',\n",
       "  4: 'GBP',\n",
       "  5: 'HKD',\n",
       "  6: 'INR',\n",
       "  7: 'ISK',\n",
       "  8: 'JPY',\n",
       "  9: 'MYR',\n",
       "  10: 'NOK',\n",
       "  11: 'SEK',\n",
       "  12: 'SGD',\n",
       "  13: 'THB',\n",
       "  14: 'TWD',\n",
       "  15: 'USD'},\n",
       " 'seniorioty_adj': {0: 'Junior Unsecured or Junior Subordinated Unsecured',\n",
       "  1: 'Secured',\n",
       "  2: 'Senior Secured',\n",
       "  3: 'Senior Subordinated Unsecured',\n",
       "  4: 'Senior Unsecured',\n",
       "  5: 'Subordinated Unsecured',\n",
       "  6: 'Unsecured'},\n",
       " 'domicile_country': {0: 'Argentina',\n",
       "  1: 'Australia',\n",
       "  2: 'Bahamas',\n",
       "  3: 'Belgium',\n",
       "  4: 'Bermuda',\n",
       "  5: 'Canada',\n",
       "  6: 'Cayman Islands',\n",
       "  7: 'China',\n",
       "  8: 'Czech Republic',\n",
       "  9: 'Greece',\n",
       "  10: 'Hong Kong',\n",
       "  11: 'Iceland',\n",
       "  12: 'India',\n",
       "  13: 'Indonesia',\n",
       "  14: 'Japan',\n",
       "  15: 'Luxembourg',\n",
       "  16: 'Malaysia',\n",
       "  17: 'Mongolia',\n",
       "  18: 'Philippines',\n",
       "  19: 'Poland',\n",
       "  20: 'Singapore',\n",
       "  21: 'South Africa',\n",
       "  22: 'South Korea',\n",
       "  23: 'Taiwan',\n",
       "  24: 'Thailand',\n",
       "  25: 'United Kingdom',\n",
       "  26: 'United States'},\n",
       " 'exchange_country': {0: 'Australia',\n",
       "  1: 'China',\n",
       "  2: 'Hong Kong',\n",
       "  3: 'India',\n",
       "  4: 'Indonesia',\n",
       "  5: 'Japan',\n",
       "  6: 'Malaysia',\n",
       "  7: 'Philippines',\n",
       "  8: 'Singapore',\n",
       "  9: 'South Korea',\n",
       "  10: 'Taiwan',\n",
       "  11: 'Thailand',\n",
       "  12: 'United States'},\n",
       " 'Industry_sector': {0: 'Communications',\n",
       "  1: 'Consumer Discretionary',\n",
       "  2: 'Consumer Staples',\n",
       "  3: 'Energy',\n",
       "  4: 'Financials',\n",
       "  5: 'Health Care',\n",
       "  6: 'Industrials',\n",
       "  7: 'Materials',\n",
       "  8: 'Real Estate',\n",
       "  9: 'Technology',\n",
       "  10: 'Utilities'},\n",
       " 'Industry_group': {0: 'Banking',\n",
       "  1: 'Consumer Discretionary Products',\n",
       "  2: 'Consumer Discretionary Services',\n",
       "  3: 'Consumer Staple Products',\n",
       "  4: 'Health Care',\n",
       "  5: 'Industrial Products',\n",
       "  6: 'Industrial Services',\n",
       "  7: 'Insurance',\n",
       "  8: 'Materials',\n",
       "  9: 'Media',\n",
       "  10: 'Oil & Gas',\n",
       "  11: 'Real Estate',\n",
       "  12: 'Renewable Energy',\n",
       "  13: 'Retail & Wholesale - Staples',\n",
       "  14: 'Retail & Whsle - Discretionary',\n",
       "  15: 'Software & Tech Services',\n",
       "  16: 'Tech Hardware & Semiconductors',\n",
       "  17: 'Telecommunications',\n",
       "  18: 'Utilities'},\n",
       " 'Industry_subgroup': {0: 'Advertising & Marketing',\n",
       "  1: 'Apparel & Textile Products',\n",
       "  2: 'Automotive',\n",
       "  3: 'Banking',\n",
       "  4: 'Beverages',\n",
       "  5: 'Biotech & Pharma',\n",
       "  6: 'Cable & Satellite',\n",
       "  7: 'Chemicals',\n",
       "  8: 'Commercial Support Services',\n",
       "  9: 'Construction Materials',\n",
       "  10: 'Consumer Services',\n",
       "  11: 'Containers & Packaging',\n",
       "  12: 'E-Commerce Discretionary',\n",
       "  13: 'Electric Utilities',\n",
       "  14: 'Electrical Equipment',\n",
       "  15: 'Engineering & Construction',\n",
       "  16: 'Entertainment Content',\n",
       "  17: 'Food',\n",
       "  18: 'Forestry, Paper & Wood Products',\n",
       "  19: 'Gas & Water Utilities',\n",
       "  20: 'Health Care Facilities & Svcs',\n",
       "  21: 'Home & Office Products',\n",
       "  22: 'Home Construction',\n",
       "  23: 'Household Products',\n",
       "  24: 'Industrial Intermediate Prod',\n",
       "  25: 'Industrial Support Services',\n",
       "  26: 'Insurance',\n",
       "  27: 'Leisure Facilities & Services',\n",
       "  28: 'Leisure Products',\n",
       "  29: 'Machinery',\n",
       "  30: 'Medical Equipment & Devices',\n",
       "  31: 'Metals & Mining',\n",
       "  32: 'Oil & Gas Producers',\n",
       "  33: 'Oil & Gas Services & Equip',\n",
       "  34: 'Publishing & Broadcasting',\n",
       "  35: 'REIT',\n",
       "  36: 'Real Estate Owners & Developers',\n",
       "  37: 'Real Estate Services',\n",
       "  38: 'Renewable Energy',\n",
       "  39: 'Retail - Consumer Staples',\n",
       "  40: 'Retail - Discretionary',\n",
       "  41: 'Semiconductors',\n",
       "  42: 'Software',\n",
       "  43: 'Steel',\n",
       "  44: 'Technology Hardware',\n",
       "  45: 'Technology Services',\n",
       "  46: 'Telecommunications',\n",
       "  47: 'Transportation & Logistics',\n",
       "  48: 'Transportation Equipment',\n",
       "  49: 'Wholesale - Consumer Staples',\n",
       "  50: 'Wholesale - Discretionary'},\n",
       " 'event_type': {0: 'Bankruptcy Filing',\n",
       "  1: 'Default Corp Action',\n",
       "  2: 'Delisting'},\n",
       " 'event_type_subcategory_sum': {0: 'Bankruptcy',\n",
       "  1: 'Debt Restructuring',\n",
       "  2: 'Insolvency',\n",
       "  3: 'Liquidation',\n",
       "  4: 'Missing Coupon & principal payment',\n",
       "  5: 'Missing Coupon payment only',\n",
       "  6: 'Missing Interest payment',\n",
       "  7: 'Missing Loan payment',\n",
       "  8: 'Missing Principal payment',\n",
       "  9: 'Others',\n",
       "  10: 'Pre-Negotiated Chapter 11',\n",
       "  11: 'Protection',\n",
       "  12: 'Receivership',\n",
       "  13: 'Rehabilitation',\n",
       "  14: 'Restructuring'},\n",
       " 'defaulted_in_last_5_years': {0: False, 1: True},\n",
       " 'defaulted_in_last_6_months': {0: False, 1: True}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency</th>\n",
       "      <th>seniorioty_adj</th>\n",
       "      <th>coupon rate</th>\n",
       "      <th>domicile_country</th>\n",
       "      <th>exchange_country</th>\n",
       "      <th>Industry_sector</th>\n",
       "      <th>Industry_group</th>\n",
       "      <th>Industry_subgroup</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_type_subcategory_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>PD_55_pd</th>\n",
       "      <th>PD_56_pd</th>\n",
       "      <th>PD_57_pd</th>\n",
       "      <th>PD_58_pd</th>\n",
       "      <th>PD_59_pd</th>\n",
       "      <th>PD_60_pd</th>\n",
       "      <th>DTD</th>\n",
       "      <th>NI_Over_TA</th>\n",
       "      <th>Size</th>\n",
       "      <th>defaulted_in_last_6_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396731</td>\n",
       "      <td>0.397453</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.398819</td>\n",
       "      <td>0.399467</td>\n",
       "      <td>0.400092</td>\n",
       "      <td>-0.732815</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.852484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5.750</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957454</td>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.957480</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.957503</td>\n",
       "      <td>0.957514</td>\n",
       "      <td>-1.666262</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-1.186347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5.675</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568169</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.125</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568169</td>\n",
       "      <td>0.568693</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.569682</td>\n",
       "      <td>0.570150</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>-1.853366</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.053677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1.750</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130285</td>\n",
       "      <td>0.130688</td>\n",
       "      <td>0.131081</td>\n",
       "      <td>0.131465</td>\n",
       "      <td>0.131840</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>-0.768857</td>\n",
       "      <td>-0.028058</td>\n",
       "      <td>-1.946507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081004</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4.950</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081004</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5.150</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081004</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5.050</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081004</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4.850</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081004</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.082676</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.954865</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>2.497169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      currency  seniorioty_adj  coupon rate  domicile_country  \\\n",
       "0           15               3        9.000                26   \n",
       "1           15               3        5.750                26   \n",
       "2           15               6        5.675                22   \n",
       "3            1               6        0.125                22   \n",
       "4            8               6        1.750                14   \n",
       "...        ...             ...          ...               ...   \n",
       "1720         9               2        5.000                16   \n",
       "1721         9               2        4.950                16   \n",
       "1722         9               2        5.150                16   \n",
       "1723         9               2        5.050                16   \n",
       "1724         9               2        4.850                16   \n",
       "\n",
       "      exchange_country  Industry_sector  Industry_group  Industry_subgroup  \\\n",
       "0                   12                1              14                 12   \n",
       "1                   12                5               4                 20   \n",
       "2                    9                1              14                 50   \n",
       "3                    9                1              14                 50   \n",
       "4                    5                6               5                 14   \n",
       "...                ...              ...             ...                ...   \n",
       "1720                 6                1               1                  2   \n",
       "1721                 6                1               1                  2   \n",
       "1722                 6                1               1                  2   \n",
       "1723                 6                1               1                  2   \n",
       "1724                 6                1               1                  2   \n",
       "\n",
       "      event_type  event_type_subcategory_sum  ...  PD_55_pd  PD_56_pd  \\\n",
       "0              0                           1  ...  0.396731  0.397453   \n",
       "1              1                           5  ...  0.957454  0.957467   \n",
       "2              1                           6  ...  0.568169  0.568693   \n",
       "3              1                           6  ...  0.568169  0.568693   \n",
       "4              0                          13  ...  0.130285  0.130688   \n",
       "...          ...                         ...  ...       ...       ...   \n",
       "1720           0                           9  ...  0.081004  0.081844   \n",
       "1721           0                           9  ...  0.081004  0.081844   \n",
       "1722           0                           9  ...  0.081004  0.081844   \n",
       "1723           0                           9  ...  0.081004  0.081844   \n",
       "1724           0                           9  ...  0.081004  0.081844   \n",
       "\n",
       "      PD_57_pd  PD_58_pd  PD_59_pd  PD_60_pd       DTD  NI_Over_TA      Size  \\\n",
       "0     0.398148  0.398819  0.399467  0.400092 -0.732815   -0.007137 -0.852484   \n",
       "1     0.957480  0.957492  0.957503  0.957514 -1.666262   -0.000286 -1.186347   \n",
       "2     0.569197  0.569682  0.570150  0.570600 -1.853366    0.000191  1.053677   \n",
       "3     0.569197  0.569682  0.570150  0.570600 -1.853366    0.000191  1.053677   \n",
       "4     0.131081  0.131465  0.131840  0.132206 -0.768857   -0.028058 -1.946507   \n",
       "...        ...       ...       ...       ...       ...         ...       ...   \n",
       "1720  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425  2.497169   \n",
       "1721  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425  2.497169   \n",
       "1722  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425  2.497169   \n",
       "1723  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425  2.497169   \n",
       "1724  0.082676  0.083500  0.084315  0.085123  0.954865    0.000425  2.497169   \n",
       "\n",
       "      defaulted_in_last_6_months  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "...                          ...  \n",
       "1720                           0  \n",
       "1721                           0  \n",
       "1722                           0  \n",
       "1723                           0  \n",
       "1724                           0  \n",
       "\n",
       "[1725 rows x 164 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1293, 164)\n",
      "Training Labels Shape: (1293,)\n",
      "Testing Features Shape: (432, 164)\n",
      "Testing Labels Shape: (432,)\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test set\n",
    "test_size = 0.25\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# Prepare the ColumnTransformer\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), non_category_features)   # StandardScaler()\n",
    "    ],\n",
    "    remainder='passthrough'  # Leave categorical features untouched\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUNING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    category_offsets: Optional[Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_numerical: int,\n",
    "        categories: Optional[List[int]],\n",
    "        d_token: int,\n",
    "        bias: bool,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if categories is None:\n",
    "            d_bias = d_numerical\n",
    "            self.category_offsets = None\n",
    "            self.category_embeddings = None\n",
    "        else:\n",
    "            d_bias = d_numerical + len(categories)\n",
    "            \n",
    "            # to ensure proper indexing for embeddings\n",
    "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
    "            self.register_buffer('category_offsets', category_offsets)\n",
    "            \n",
    "            # sum(categories) tensors of size d_token\n",
    "            self.category_embeddings = nn.Embedding(sum(categories), d_token)\n",
    "            \n",
    "            # initialize the embeddings\n",
    "            nn_init.kaiming_uniform_(self.category_embeddings.weight, a=math.sqrt(5))\n",
    "            print(f'{self.category_embeddings.weight.shape=}')\n",
    "\n",
    "        # take [CLS] token into account\n",
    "        self.weight = nn.Parameter(Tensor(d_numerical + 1, d_token))\n",
    "        self.bias = nn.Parameter(Tensor(d_bias, d_token)) if bias else None\n",
    "        # The initialization is inspired by nn.Linear\n",
    "        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self) -> int:\n",
    "        return len(self.weight) + (\n",
    "            0 if self.category_offsets is None else len(self.category_offsets)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "        x_some = x_num if x_cat is None else x_cat\n",
    "        assert x_some is not None\n",
    "        \n",
    "        # if x_cat is not None, x_num = x_cat + x_num\n",
    "        x_num = torch.cat(\n",
    "            [torch.ones(len(x_some), 1, device=x_some.device)]  # [CLS]\n",
    "            + ([] if x_num is None else [x_num]),\n",
    "            dim=1,\n",
    "        )\n",
    "        \n",
    "        # numerical features are weighted by weights\n",
    "        x = self.weight[None] * x_num[:, :, None]\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat(\n",
    "                [x, self.category_embeddings(x_cat + self.category_offsets[None])],\n",
    "                dim=1,\n",
    "            )\n",
    "        if self.bias is not None:\n",
    "            bias = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(1, self.bias.shape[1], device=x.device),\n",
    "                    self.bias,\n",
    "                ]\n",
    "            )\n",
    "            x = x + bias[None]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d: int, n_heads: int, dropout: float, initialization: str\n",
    "    ) -> None:\n",
    "        if n_heads > 1:\n",
    "            assert d % n_heads == 0\n",
    "        assert initialization in ['xavier', 'kaiming']\n",
    "\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d, d)\n",
    "        self.W_k = nn.Linear(d, d)\n",
    "        self.W_v = nn.Linear(d, d)\n",
    "        self.W_out = nn.Linear(d, d) if n_heads > 1 else None\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "\n",
    "        for m in [self.W_q, self.W_k, self.W_v]:\n",
    "            if initialization == 'xavier' and (n_heads > 1 or m is not self.W_v):\n",
    "                # gain is needed since W_qkv is represented with 3 separate layers\n",
    "                nn_init.xavier_uniform_(m.weight, gain=1 / math.sqrt(2))\n",
    "            nn_init.zeros_(m.bias)\n",
    "        if self.W_out is not None:\n",
    "            nn_init.zeros_(self.W_out.bias)\n",
    "\n",
    "    def _reshape(self, x: Tensor) -> Tensor:\n",
    "        batch_size, n_tokens, d = x.shape\n",
    "        d_head = d // self.n_heads\n",
    "        return (\n",
    "            x.reshape(batch_size, n_tokens, self.n_heads, d_head)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(batch_size * self.n_heads, n_tokens, d_head)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_q: Tensor,\n",
    "        x_kv: Tensor,\n",
    "        key_compression: Optional[nn.Linear],\n",
    "        value_compression: Optional[nn.Linear],\n",
    "    ) -> Tensor:\n",
    "        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n",
    "        for tensor in [q, k, v]:\n",
    "            assert tensor.shape[-1] % self.n_heads == 0\n",
    "        if key_compression is not None:\n",
    "            assert value_compression is not None\n",
    "            k = key_compression(k.transpose(1, 2)).transpose(1, 2)\n",
    "            v = value_compression(v.transpose(1, 2)).transpose(1, 2)\n",
    "        else:\n",
    "            assert value_compression is None\n",
    "\n",
    "        batch_size = len(q)\n",
    "        d_head_key = k.shape[-1] // self.n_heads\n",
    "        d_head_value = v.shape[-1] // self.n_heads\n",
    "        n_q_tokens = q.shape[1]\n",
    "\n",
    "        q = self._reshape(q)\n",
    "        k = self._reshape(k)\n",
    "        attention = F.softmax(q @ k.transpose(1, 2) / math.sqrt(d_head_key), dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            attention = self.dropout(attention)\n",
    "        x = attention @ self._reshape(v)\n",
    "        x = (\n",
    "            x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n",
    "        )\n",
    "        if self.W_out is not None:\n",
    "            x = self.W_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(name: str) -> Callable[[Tensor], Tensor]:\n",
    "    return (\n",
    "        reglu\n",
    "        if name == 'reglu'\n",
    "        else geglu\n",
    "        if name == 'geglu'\n",
    "        else torch.sigmoid\n",
    "        if name == 'sigmoid'\n",
    "        else getattr(F, name)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_nonglu_activation_fn(name: str) -> Callable[[Tensor], Tensor]:\n",
    "    return (\n",
    "        F.relu\n",
    "        if name == 'reglu'\n",
    "        else F.gelu\n",
    "        if name == 'geglu'\n",
    "        else get_activation_fn(name)\n",
    "    )\n",
    "    \n",
    "def reglu(x: Tensor) -> Tensor:\n",
    "    a, b = x.chunk(2, dim=-1)\n",
    "    return a * F.relu(b)\n",
    "\n",
    "\n",
    "def geglu(x: Tensor) -> Tensor:\n",
    "    a, b = x.chunk(2, dim=-1)\n",
    "    return a * F.gelu(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer.\n",
    "\n",
    "    References:\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "    - https://github.com/facebookresearch/pytext/tree/master/pytext/models/representations/transformer\n",
    "    - https://github.com/pytorch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/linformer/linformer_src/modules/multihead_linear_attention.py#L19\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        # tokenizer\n",
    "        d_numerical: int,\n",
    "        categories: Optional[List[int]],\n",
    "        token_bias: bool,\n",
    "        # transformer\n",
    "        n_layers: int,\n",
    "        d_token: int,\n",
    "        n_heads: int,\n",
    "        d_ffn_factor: float,\n",
    "        attention_dropout: float,\n",
    "        ffn_dropout: float,\n",
    "        residual_dropout: float,\n",
    "        activation: str,\n",
    "        prenormalization: bool,\n",
    "        initialization: str,\n",
    "        # linformer\n",
    "        kv_compression: Optional[float],\n",
    "        kv_compression_sharing: Optional[str],\n",
    "        #\n",
    "        d_out: int,\n",
    "    ) -> None:\n",
    "        assert (kv_compression is None) ^ (kv_compression_sharing is not None)\n",
    "\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(d_numerical, categories, d_token, token_bias)\n",
    "        n_tokens = self.tokenizer.n_tokens\n",
    "\n",
    "        def make_kv_compression():\n",
    "            assert kv_compression\n",
    "            compression = nn.Linear(\n",
    "                n_tokens, int(n_tokens * kv_compression), bias=False\n",
    "            )\n",
    "            if initialization == 'xavier':\n",
    "                nn_init.xavier_uniform_(compression.weight)\n",
    "            return compression\n",
    "\n",
    "        self.shared_kv_compression = (\n",
    "            make_kv_compression()\n",
    "            if kv_compression and kv_compression_sharing == 'layerwise'\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        def make_normalization():\n",
    "            return nn.LayerNorm(d_token)\n",
    "\n",
    "        d_hidden = int(d_token * d_ffn_factor)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for layer_idx in range(n_layers):\n",
    "            layer = nn.ModuleDict(\n",
    "                {\n",
    "                    'attention': MultiheadAttention(\n",
    "                        d_token, n_heads, attention_dropout, initialization\n",
    "                    ),\n",
    "                    'linear0': nn.Linear(\n",
    "                        d_token, d_hidden * (2 if activation.endswith('glu') else 1)\n",
    "                    ),\n",
    "                    'linear1': nn.Linear(d_hidden, d_token),\n",
    "                    'norm1': make_normalization(),\n",
    "                }\n",
    "            )\n",
    "            if not prenormalization or layer_idx:\n",
    "                layer['norm0'] = make_normalization()\n",
    "            if kv_compression and self.shared_kv_compression is None:\n",
    "                layer['key_compression'] = make_kv_compression()\n",
    "                if kv_compression_sharing == 'headwise':\n",
    "                    layer['value_compression'] = make_kv_compression()\n",
    "                else:\n",
    "                    assert kv_compression_sharing == 'key-value'\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.activation = get_activation_fn(activation)\n",
    "        self.last_activation = get_nonglu_activation_fn(activation)\n",
    "        self.prenormalization = prenormalization\n",
    "        self.last_normalization = make_normalization() if prenormalization else None\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.residual_dropout = residual_dropout\n",
    "        self.head = nn.Linear(d_token, d_out)\n",
    "\n",
    "    def _get_kv_compressions(self, layer):\n",
    "        return (\n",
    "            (self.shared_kv_compression, self.shared_kv_compression)\n",
    "            if self.shared_kv_compression is not None\n",
    "            else (layer['key_compression'], layer['value_compression'])\n",
    "            if 'key_compression' in layer and 'value_compression' in layer\n",
    "            else (layer['key_compression'], layer['key_compression'])\n",
    "            if 'key_compression' in layer\n",
    "            else (None, None)\n",
    "        )\n",
    "\n",
    "    def _start_residual(self, x, layer, norm_idx):\n",
    "        x_residual = x\n",
    "        if self.prenormalization:\n",
    "            norm_key = f'norm{norm_idx}'\n",
    "            if norm_key in layer:\n",
    "                x_residual = layer[norm_key](x_residual)\n",
    "        return x_residual\n",
    "\n",
    "    def _end_residual(self, x, x_residual, layer, norm_idx):\n",
    "        if self.residual_dropout:\n",
    "            x_residual = F.dropout(x_residual, self.residual_dropout, self.training)\n",
    "        x = x + x_residual\n",
    "        if not self.prenormalization:\n",
    "            x = layer[f'norm{norm_idx}'](x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: Optional[Tensor]) -> Tensor:\n",
    "        x = self.tokenizer(x_num, x_cat)\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            is_last_layer = layer_idx + 1 == len(self.layers)\n",
    "            layer = cast(Dict[str, nn.Module], layer)\n",
    "\n",
    "            # start residual connection\n",
    "            x_residual = self._start_residual(x, layer, 0)\n",
    "            \n",
    "            # attention layer\n",
    "            x_residual = layer['attention'](\n",
    "                # for the last attention, it is enough to process only [CLS]\n",
    "                (x_residual[:, :1] if is_last_layer else x_residual),\n",
    "                x_residual,\n",
    "                *self._get_kv_compressions(layer),\n",
    "            )\n",
    "            \n",
    "            # end residual connection\n",
    "            if is_last_layer:\n",
    "                x = x[:, : x_residual.shape[1]]\n",
    "            x = self._end_residual(x, x_residual, layer, 0)\n",
    "\n",
    "            # feedforward network\n",
    "            x_residual = self._start_residual(x, layer, 1)\n",
    "            x_residual = layer['linear0'](x_residual)\n",
    "            x_residual = self.activation(x_residual)\n",
    "            if self.ffn_dropout:\n",
    "                x_residual = F.dropout(x_residual, self.ffn_dropout, self.training)\n",
    "            x_residual = layer['linear1'](x_residual)\n",
    "            x = self._end_residual(x, x_residual, layer, 1)\n",
    "\n",
    "        # final layer processing\n",
    "        assert x.shape[1] == 1\n",
    "        x = x[:, 0]\n",
    "        if self.last_normalization is not None:\n",
    "            x = self.last_normalization(x)\n",
    "        x = self.last_activation(x)\n",
    "        x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 7, 27, 13, 11, 19, 51, 3, 15, 2, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(le.classes_) for le in label_encoders.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    # model = define_model(trial, train_features.shape[1], 1).to(DEVICE)\n",
    "    # Define out_features_list\n",
    "    n_heads = trial.suggest_int(\"n_heads\", 1, 10)\n",
    "    \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    \n",
    "    token_multiplier = trial.suggest_int(\"token_multiplier\", 5, 30)  # Adjust the range as necessary\n",
    "    \n",
    "    # Suggest an integer for a that is divisible by b\n",
    "    d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n",
    "    \n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0, 0.5)\n",
    "    d_ffn_factor = trial.suggest_float(\"d_ffn_factor\", 1, 3)\n",
    "    ffn_dropout = trial.suggest_float(\"ffn_dropout\", 0, 0.5)\n",
    "    \n",
    "    # activation = trial.suggest_categorical(\"activation\", choices=['relu', 'reglu', 'geglu'])\n",
    "    # batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
    "    \n",
    "    categories = [len(le.classes_) for le in label_encoders.values()]\n",
    "    d_numerical = len(non_category_features)\n",
    "\n",
    "    args = {'activation': 'relu', #activation, #\n",
    "    'attention_dropout': attention_dropout,\n",
    "    'd_ffn_factor': d_ffn_factor,\n",
    "    'd_token': d_token,\n",
    "    'ffn_dropout': ffn_dropout,\n",
    "    'initialization': 'kaiming',\n",
    "    'n_heads': n_heads,\n",
    "    'n_layers': n_layers,\n",
    "    'prenormalization': False,\n",
    "    'residual_dropout': 0.0,\n",
    "    'kv_compression': None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    'token_bias': True,\n",
    "    'd_out': 1\n",
    "    }\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-9, 1e-1, log=True)\n",
    "\n",
    "    # training with 5-fold CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_features):\n",
    "        # Create training and validation datasets for the current fold\n",
    "        X_train_fold, X_val_fold = train_features.iloc[train_idx], train_features.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = train_labels.iloc[train_idx], train_labels.iloc[val_idx]\n",
    "        \n",
    "        # scaling features\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "            \n",
    "        # Initialize the model for this fold\n",
    "        model = Transformer(d_numerical=d_numerical, categories=categories, **args).to(DEVICE)\n",
    "        model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # define optimizer\n",
    "        if optimizer_name == \"Adam\":\n",
    "         optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            momentum = trial.suggest_float(\"momentum\", 1e-9, 0.95, log=True)\n",
    "            optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        \n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Prepare DataLoader for training\n",
    "        train_dataset = CustomDataset(X_train_fold, y_train_fold.to_numpy())\n",
    "        val_dataset = CustomDataset(X_val_fold, y_val_fold.to_numpy())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            \n",
    "        # Training of the model.\n",
    "        model.train()\n",
    "        for epoch in range(EPOCHS):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                \n",
    "                X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "                X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_num, X_cat)\n",
    "\n",
    "                # print(\"shape\", output.shape, target.shape)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "                X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "                output = model(X_num, X_cat)\n",
    "                val_loss = criterion(output, target).item()\n",
    "                val_losses.append(val_loss**0.5) #rmse\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "    # Return the average validation loss across all folds\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-27 03:59:38,680] A new study created in memory with name: no-name-e5f08483-18f9-496d-961d-f3f73e36a323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63241/1311913245.py:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 5])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-08-27 04:03:49,651] Trial 0 finished with value: 0.2778907221754452 and parameters: {'n_heads': 5, 'n_layers': 3, 'token_multiplier': 13, 'd_token': 5, 'attention_dropout': 0.47357853391284443, 'd_ffn_factor': 1.5567463702685527, 'ffn_dropout': 0.09703116655822941, 'optimizer': 'RMSprop', 'lr': 0.00045523806322604355, 'weight_decay': 5.01534556567381e-07, 'momentum': 8.456152924489299e-07}. Best is trial 0 with value: 0.2778907221754452.\n",
      "/tmp/ipykernel_63241/1311913245.py:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 40])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-08-27 04:07:01,453] Trial 1 finished with value: 0.3261027943937555 and parameters: {'n_heads': 10, 'n_layers': 3, 'token_multiplier': 13, 'd_token': 40, 'attention_dropout': 0.11634236810833715, 'd_ffn_factor': 1.4151651199217234, 'ffn_dropout': 0.20928743701569774, 'optimizer': 'Adam', 'lr': 0.009410155719751772, 'weight_decay': 0.09667794268657083}. Best is trial 0 with value: 0.2778907221754452.\n",
      "/tmp/ipykernel_63241/1311913245.py:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  d_token = trial.suggest_int(\"d_token\", n_heads, n_heads * token_multiplier, n_heads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenhoa/miniconda3/envs/.henv/lib/python3.12/site-packages/optuna/trial/_trial.py:493: UserWarning: The reported value is ignored because this `step` 49 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-08-27 04:10:04,882] Trial 2 finished with value: 0.2516675406673338 and parameters: {'n_heads': 5, 'n_layers': 2, 'token_multiplier': 20, 'd_token': 60, 'attention_dropout': 0.09192629672301816, 'd_ffn_factor': 2.565011542335736, 'ffn_dropout': 0.07564072635765423, 'optimizer': 'RMSprop', 'lr': 0.00022923558072776028, 'weight_decay': 7.433796546124456e-05, 'momentum': 2.161429700687457e-05}. Best is trial 2 with value: 0.2516675406673338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  3\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  3\n",
      "Best trial:\n",
      "  Value:  0.2516675406673338\n",
      "  Params: \n",
      "    n_heads: 5\n",
      "    n_layers: 2\n",
      "    token_multiplier: 20\n",
      "    d_token: 60\n",
      "    attention_dropout: 0.09192629672301816\n",
      "    d_ffn_factor: 2.565011542335736\n",
      "    ffn_dropout: 0.07564072635765423\n",
      "    optimizer: RMSprop\n",
      "    lr: 0.00022923558072776028\n",
      "    weight_decay: 7.433796546124456e-05\n",
      "    momentum: 2.161429700687457e-05\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_heads': 5,\n",
       " 'n_layers': 2,\n",
       " 'token_multiplier': 20,\n",
       " 'd_token': 60,\n",
       " 'attention_dropout': 0.09192629672301816,\n",
       " 'd_ffn_factor': 2.565011542335736,\n",
       " 'ffn_dropout': 0.07564072635765423,\n",
       " 'optimizer': 'RMSprop',\n",
       " 'lr': 0.00022923558072776028,\n",
       " 'weight_decay': 7.433796546124456e-05,\n",
       " 'momentum': 2.161429700687457e-05}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\"model\": {}, \"optimizer\": {}}\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    if key in ['lr', 'weight_decay', 'momentum', 'weight_decay', 'optimizer']:\n",
    "        MODEL_CONFIG[\"optimizer\"][key] = value\n",
    "    elif key == 'token_multiplier':\n",
    "        continue\n",
    "    elif key == 'batch_size':\n",
    "        BATCH_SIZE = value\n",
    "    else:\n",
    "        # adj_key = key.rpartition('_')[0]\n",
    "        MODEL_CONFIG[\"model\"][key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'n_heads': 5,\n",
       "  'n_layers': 2,\n",
       "  'd_token': 60,\n",
       "  'attention_dropout': 0.09192629672301816,\n",
       "  'd_ffn_factor': 2.565011542335736,\n",
       "  'ffn_dropout': 0.07564072635765423},\n",
       " 'optimizer': {'optimizer': 'RMSprop',\n",
       "  'lr': 0.00022923558072776028,\n",
       "  'weight_decay': 7.433796546124456e-05,\n",
       "  'momentum': 2.161429700687457e-05}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "new_feature_list = non_category_features + category_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = CustomDataset(train_features, train_labels.to_numpy())\n",
    "test_dataset = CustomDataset(test_features, test_labels.to_numpy())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_features.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    }
   ],
   "source": [
    "categories = [len(le.classes_) for le in label_encoders.values()]\n",
    "d_numerical = len(non_category_features)\n",
    "# d_token = 32  # Example token dimension\n",
    "# token_bias = True\n",
    "\n",
    "args = {\n",
    "  'initialization': 'kaiming',\n",
    "  'activation': 'relu',\n",
    "    'prenormalization': False,\n",
    "    'residual_dropout': 0.0,\n",
    "    'kv_compression': None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    'token_bias': True,\n",
    "    'd_out': 1\n",
    "}\n",
    "\n",
    "args.update(MODEL_CONFIG[\"model\"])\n",
    "model = Transformer(d_numerical=d_numerical, categories=categories, **args).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tokenizer): Tokenizer(\n",
       "    (category_embeddings): Embedding(166, 60)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x ModuleDict(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_q): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (W_k): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (W_v): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (W_out): Linear(in_features=60, out_features=60, bias=True)\n",
       "        (dropout): Dropout(p=0.09192629672301816, inplace=False)\n",
       "      )\n",
       "      (linear0): Linear(in_features=60, out_features=153, bias=True)\n",
       "      (linear1): Linear(in_features=153, out_features=60, bias=True)\n",
       "      (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=60, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Transformer(\n",
       "    (tokenizer): Tokenizer(\n",
       "      (category_embeddings): Embedding(166, 60)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x ModuleDict(\n",
       "        (attention): MultiheadAttention(\n",
       "          (W_q): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (W_k): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (W_v): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (W_out): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (dropout): Dropout(p=0.09192629672301816, inplace=False)\n",
       "        )\n",
       "        (linear0): Linear(in_features=60, out_features=153, bias=True)\n",
       "        (linear1): Linear(in_features=153, out_features=60, bias=True)\n",
       "        (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm0): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (head): Linear(in_features=60, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE != \"cpu\":\n",
    "    model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.00022923558072776028\n",
       "    maximize: False\n",
       "    momentum: 2.161429700687457e-05\n",
       "    weight_decay: 7.433796546124456e-05\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define optimizer\n",
    "optim_config = deepcopy(MODEL_CONFIG[\"optimizer\"])\n",
    "del optim_config[\"optimizer\"]\n",
    "\n",
    "optimizer = getattr(optim, MODEL_CONFIG[\"optimizer\"][\"optimizer\"])(model.parameters(), **optim_config)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:35<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500], Train Loss: 0.1137\n",
      "Training time: 515.221 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 500\n",
    "criterion = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "\n",
    "for ep in tqdm(range(EPOCH)):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "        X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X_num, X_cat)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() * target.size(0)\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{ep + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "\n",
    "\n",
    "    train_loss = running_loss  / len(train_loader.dataset)\n",
    "    \n",
    "train_loss = running_loss  / len(train_loader.dataset)\n",
    "print(f'Epoch [{ep+1}], Train Loss: {train_loss**0.5:.4f}')\n",
    "\n",
    "# print out training time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training time: {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache first\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training average mean absolute error: 0.15283623337745667\n",
      "Training average mean absolute percentage error: 312.3091459274292\n",
      "Training average root mean squared error: 0.23530429349573925\n",
      "Training average R2: 0.516621470451355\n"
     ]
    }
   ],
   "source": [
    "# Testing phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for data, target in test_loader:\n",
    "        # inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        X_num = data[:, :len(non_category_features)]\n",
    "        X_cat = data[:, -len(category_features):].detach().long()\n",
    "        \n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model = model.module  # Unwrap from DataParallel\n",
    "        model = model.to('cpu')\n",
    "            \n",
    "               \n",
    "        outputs = model(X_num, X_cat)\n",
    "\n",
    "        # save metrics\n",
    "        mae, mape, rmse, rsqr = calculate_metric(outputs.numpy(), target.numpy())\n",
    "        print(f\"Training average mean absolute error: {mae}\")\n",
    "        print(f\"Training average mean absolute percentage error: {mape}\")\n",
    "        print(f\"Training average root mean squared error: {rmse}\")\n",
    "        print(f\"Training average R2: {rsqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s load back in our saved model\n",
    "# model = MLP()\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 164)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currency                        int64\n",
       "seniorioty_adj                  int64\n",
       "coupon rate                   float64\n",
       "domicile_country                int64\n",
       "exchange_country                int64\n",
       "                               ...   \n",
       "PD_60_pd                      float64\n",
       "DTD                           float64\n",
       "NI_Over_TA                    float64\n",
       "Size                          float64\n",
       "defaulted_in_last_6_months      int64\n",
       "Length: 164, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n",
      "self.category_embeddings.weight.shape=torch.Size([166, 60])\n",
      "activation <function relu at 0x7f35d2d734c0> relu\n"
     ]
    }
   ],
   "source": [
    "# Define cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "EPOCHS = 500\n",
    "val_mae = []\n",
    "val_mape = []\n",
    "val_rmse = []\n",
    "val_rsqr = []\n",
    "\n",
    "\n",
    "for train_idx, val_idx in kf.split(features):\n",
    "    # Create training and validation datasets for the current fold\n",
    "    X_train_fold, X_val_fold = features.iloc[train_idx], features.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = labels.iloc[train_idx], labels.iloc[val_idx]\n",
    "    \n",
    "    # scaling features\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "        \n",
    "    # Initialize the model for this fold\n",
    "    model = Transformer(d_numerical=d_numerical, categories=categories, **args)\n",
    "    model = nn.DataParallel(model, device_ids = DEVICE_LIST)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = getattr(optim, MODEL_CONFIG[\"optimizer\"][\"optimizer\"])(model.parameters(), **optim_config)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Prepare DataLoader for training\n",
    "    train_dataset = CustomDataset(X_train_fold, y_train_fold.to_numpy())\n",
    "    val_dataset = CustomDataset(X_val_fold, y_val_fold.to_numpy())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_dataset.features.shape[0], shuffle=True)\n",
    "        \n",
    "    # Training of the model.\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "  \n",
    "            X_num = data[:, :len(non_category_features)].to(DEVICE)\n",
    "            X_cat = data[:, -len(category_features):].detach().long().to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(X_num, X_cat)\n",
    "\n",
    "            \n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(f'Epoch [{ep+1}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Validation of the model.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            X_num = data[:, :len(non_category_features)]\n",
    "            X_cat = data[:, -len(category_features):].detach().long()\n",
    "            \n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                model = model.module  # Unwrap from DataParallel\n",
    "            model = model.to('cpu')\n",
    "            outputs = model(X_num, X_cat)\n",
    "            \n",
    "            # save metrics\n",
    "            mae, mape, rmse, rsqr = calculate_metric(outputs.numpy(), target.numpy())\n",
    "            val_mae.append(mae)\n",
    "            val_mape.append(mape)\n",
    "            val_rmse.append(rmse)\n",
    "            val_rsqr.append(rsqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average mean absolute error: 0.16323910653591156\n",
      "Test average mean absolute percentage error: 5443.055862188339\n",
      "Test average root mean squared error: 0.24957806415090034\n",
      "Test average R2: 0.42442349195480344\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test average mean absolute error: {statistics.mean(val_mae)}\")\n",
    "print(f\"Test average mean absolute percentage error: {statistics.mean(val_mape)}\")\n",
    "print(f\"Test average root mean squared error: {statistics.mean(val_rmse)}\")\n",
    "print(f\"Test average R2: {statistics.mean(val_rsqr)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
